{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Federico6419/MachineLearningProject/blob/main/MachineLearningProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jDBR7kyywx2"
      },
      "source": [
        "## Install libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0A3otcPC9xYL",
        "outputId": "b78f1986-f966-4337-9300-607dd28bf2ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: swig in /usr/local/lib/python3.10/dist-packages (4.1.1)\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.3.5)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.5.2)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.1.1)\n",
            "Requirement already satisfied: gym-notebook-wrapper in /usr/local/lib/python3.10/dist-packages (1.3.3)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (from gym-notebook-wrapper) (0.25.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from gym-notebook-wrapper) (3.7.1)\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.10/dist-packages (from gym-notebook-wrapper) (3.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from gym-notebook-wrapper) (7.34.0)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (from gym-notebook-wrapper) (1.0.3)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym->gym-notebook-wrapper) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym->gym-notebook-wrapper) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym->gym-notebook-wrapper) (0.0.8)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->gym-notebook-wrapper) (67.7.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->gym-notebook-wrapper) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->gym-notebook-wrapper) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->gym-notebook-wrapper) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->gym-notebook-wrapper) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->gym-notebook-wrapper) (3.0.39)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->gym-notebook-wrapper) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->gym-notebook-wrapper) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->gym-notebook-wrapper) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->gym-notebook-wrapper) (4.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gym-notebook-wrapper) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gym-notebook-wrapper) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gym-notebook-wrapper) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gym-notebook-wrapper) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gym-notebook-wrapper) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gym-notebook-wrapper) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gym-notebook-wrapper) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gym-notebook-wrapper) (2.8.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy->gym-notebook-wrapper) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy->gym-notebook-wrapper) (2.31.0)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy->gym-notebook-wrapper) (0.1.10)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy->gym-notebook-wrapper) (2.31.5)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy->gym-notebook-wrapper) (0.4.9)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->gym-notebook-wrapper) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->gym-notebook-wrapper) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->gym-notebook-wrapper) (0.2.8)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->gym-notebook-wrapper) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->gym-notebook-wrapper) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->gym-notebook-wrapper) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->gym-notebook-wrapper) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->gym-notebook-wrapper) (2023.7.22)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n",
            "Requirement already satisfied: xvfbwrapper in /usr/local/lib/python3.10/dist-packages (0.2.9)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n",
        "!pip install swig     #This solves the errori in the installation of gymnasium[box2d]\n",
        "!pip install gymnasium[box2d]\n",
        "!pip install gym-notebook-wrapper   #This installs Gym-Notebook-Wrapper, that provides small wrappers for running and rendering OpenAI Gym\n",
        "\n",
        "#To solve the xvfb missing file problem\n",
        "!sudo apt-get install xvfb\n",
        "!pip install xvfbwrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lCpyHIx0K0w"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IW5i1FfF0PeI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d953b102-bef2-4594-e21f-1a8c6a3d619d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MachineLearningProject'...\n",
            "remote: Enumerating objects: 172, done.\u001b[K\n",
            "remote: Counting objects: 100% (37/37), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 172 (delta 21), reused 0 (delta 0), pack-reused 135\u001b[K\n",
            "Receiving objects: 100% (172/172), 106.34 KiB | 818.00 KiB/s, done.\n",
            "Resolving deltas: 100% (98/98), done.\n",
            "/content/MachineLearningProject/MachineLearningProject\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Federico6419/MachineLearningProject          #It clones my github repository\n",
        "%cd MachineLearningProject\n",
        "\n",
        "import gymnasium as gym\n",
        "import gnwrapper\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "import cv2\n",
        "\n",
        "import config\n",
        "from model import Model\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "#from gym.wrappers import Monitor\n",
        "from gym.wrappers.record_video import RecordVideo\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else:\n",
        "    print(\"Could not find video\")\n",
        "\n",
        "\n",
        "def wrap_env(env):\n",
        "  #env = Monitor(env, './video', force=True)\n",
        "  env = RecordVideo(env, './video',  episode_trigger = lambda episode_number: True)\n",
        "  return env"
      ],
      "metadata": {
        "id": "cGzNFk2bPMlx",
        "outputId": "5a1dd24b-af7c-4f8d-da28-a45482d49c04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# F"
      ],
      "metadata": {
        "id": "MKJKM9g2pV6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "episode_reward = 0\n",
        "#Add negative reward?\n",
        "buffer = deque([], config.BUFFER_SIZE)             #Initialize the Queue that contains the past experience\n",
        "epsilon = config.MAX_EPSILON\n",
        "alpha = config.ALPHA\n",
        "decay = config.EPSILON_DECAY\n",
        "\n",
        "#For the plotting\n",
        "cum_reward_table = np.zeros(config.NUM_EPISODES)\n",
        "cum_reward_nn = np.zeros(config.NUM_EPISODES)\n",
        "\n",
        "#Initialize the Model\n",
        "model = Model().to(config.DEVICE)\n",
        "\n",
        "#Initialize the Target Model\n",
        "target_model = Model().to(config.DEVICE)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=config.LR)\n",
        "optimizer_target = optim.Adam(target_model.parameters(), lr=config.LR)\n",
        "\n",
        "huber_loss=nn.HuberLoss(delta=1.0)\n",
        "\n",
        "action_space = [\n",
        "                (-1, 1, 0.2), (0, 1, 0.2), (1, 1, 0.2),\n",
        "                (-1, 1,   0), (0, 1,   0), (1, 1,   0),               #(Steering Wheel, Gas, Break)\n",
        "                (-1, 0, 0.2), (0, 0, 0.2), (1, 0, 0.2),               #Range -1~1 0~1 0~1\n",
        "                (-1, 0,   0), (0, 0,   0), (1, 0,   0)\n",
        "              ]\n",
        "\n",
        "#Define the policy to know how chose the action\n",
        "#Q-Table\n",
        "def select_action(state, epsilon):\n",
        "    rv = random.uniform(0, 1)\n",
        "    if rv < epsilon:\n",
        "        return env.action_space.sample()\n",
        "    else:\n",
        "        return np.argmax(Q[state])\n",
        "\n",
        "#Neural Network\n",
        "def select_action_nn(state, epsilon):\n",
        "    rv = random.uniform(0, 1)\n",
        "    if rv < epsilon:\n",
        "        return env.action_space.sample(), random.randrange(len(action_space))          #We sample a random action\n",
        "\n",
        "    else:\n",
        "        prediction = model(torch.from_numpy(state.astype('float32')).to(config.DEVICE)).detach().cpu().numpy()\n",
        "        action=np.argmax(prediction)              #Select the action with the maximum predicted Q-Value\n",
        "\n",
        "        return prediction, action\n",
        "\n",
        "\n",
        "## update the epsilon value along the iteration until converges to MIN_EPSILON\n",
        "def update_epsilon(epsilon):\n",
        "    epsilon -= epsilon/100 # reduce epsilon by 1/100\n",
        "    if epsilon<=config.MIN_EPSILON:\n",
        "        return config.MIN_EPSILON\n",
        "    else:\n",
        "        return epsilon\n",
        "\n",
        "## update the epsilon every episode by epsilon decay variable\n",
        "def update_epsilon_nn(epsilon):\n",
        "    epsilon *= decay\n",
        "    if epsilon<=config.MIN_EPSILON:\n",
        "        return config.MIN_EPSILON\n",
        "    else:\n",
        "        return epsilon\n",
        "\n",
        "\n",
        "env = gym.make(\"CarRacing-v2\", render_mode=\"human\")\n",
        "\n",
        "\n",
        "if(config.USE_QTABLE):\n",
        "    # define the Q table\n",
        "    #Q = np.zeros([27684, env.action_space.n]) # little discretization\n",
        "    Q = np.zeros([19051200, env.action_space.n]) #big discretization\n",
        "\n",
        "###see the limit of the values of the box observation space\n",
        "#print(env.observation_space.high)\n",
        "#print(env.observation_space.low)\n",
        "\n",
        "###see in more detail the action space and the observation space\n",
        "#print(env.action_space)\n",
        "#print(env.observation_space)\n",
        "\n",
        "\n",
        "if(config.USE_QTABLE): # use a q table to reach the goal\n",
        "    for i in range(config.NUM_EPISODES):\n",
        "        observation, info = env.reset()# use seed to have same initial state\n",
        "        #state = config.discretize(observation)\n",
        "        state = config.big_discretize(observation)\n",
        "\n",
        "        for j in range(500):\n",
        "            action = select_action(state,epsilon)\n",
        "            obv, reward, done, truncated, info = env.step(action)\n",
        "            #next_state = config.discretize(obv)\n",
        "            next_state = config.big_discretize(obv)\n",
        "\n",
        "            next_max = np.max(Q[next_state])\n",
        "\n",
        "            Q[state,action] += alpha*(reward+config.GAMMA*next_max-Q[state,action])\n",
        "            state = next_state\n",
        "\n",
        "            episode_reward += reward\n",
        "\n",
        "            if done or truncated:\n",
        "                break\n",
        "\n",
        "        print(\"episode: \", i)\n",
        "        print(\"episode cumulative reward : \", episode_reward)\n",
        "        print(\"epsilon: \",epsilon)\n",
        "        epsilon = update_epsilon(epsilon)\n",
        "        cum_reward_table[i]=episode_reward\n",
        "        episode_reward = 0 #reset the total reward each episode\n",
        "\n",
        "    #save the q table for testing\n",
        "    #np.savetxt('q_table.csv', Q, delimiter=','fmt='%f18')\n",
        "    #np.savetxt('q_table_little_discretization2000.csv', Q, delimiter=',') # full precision\n",
        "    np.savetxt('q_table_big_discretization1000.csv', Q, delimiter=',') # full precision\n",
        "\n",
        "else:             #Use a Neural Network to approximate the Q Function\n",
        "    for i in range(config.NUM_EPISODES):\n",
        "        state, info = env.reset()               #The state is a 96x96 Matrix, that contains elements composed by 3 Colours RGB\n",
        "        state = cv2.cvtColor(state, cv2.COLOR_BGR2GRAY)           #Convert the state into a Grayscale Image, that is a Matrix 96x96 composed by Integer values\n",
        "        #Add normalization?\n",
        "\n",
        "        frames_queue = deque([state]*3, maxlen = 3)\n",
        "\n",
        "        done = False\n",
        "\n",
        "        #for j in range(500):\n",
        "        j = 0\n",
        "        while(True):\n",
        "\n",
        "            #ATTENZIONE, NON SO SE CONVERTIRE IN NP ARRAY E POI IN TENSOR SIA GIUSTO\n",
        "            current_frame = np.array(frames_queue)\n",
        "            #current_frame = torch.from_numpy(current_frame)\n",
        "            #current_frame = np.transpose(current_frame, (1, 2, 0))\n",
        "\n",
        "            #action = select_action_nn(state, epsilon)\n",
        "            action, action_id = select_action_nn(current_frame, epsilon)                      #The Action is composed by 3 Values, that are the steering, gas and breaking\n",
        "\n",
        "            rew = 0\n",
        "            #Skip Frames\n",
        "            for tot in range(3):\n",
        "                next_state, reward, done, truncated, info = env.step(action)\n",
        "                rew += reward\n",
        "                if done or truncated:\n",
        "                    break\n",
        "\n",
        "            #next_state, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "            episode_reward += rew\n",
        "\n",
        "            next_state = cv2.cvtColor(next_state, cv2.COLOR_BGR2GRAY)\n",
        "            #Add normalization?\n",
        "\n",
        "            #ATTENZIONE, NON SO SE CONVERTIRE IN NP ARRAY E POI IN TENSOR SIA GIUSTO\n",
        "            frames_queue.append(next_state)\n",
        "            next_frame = np.array(frames_queue)\n",
        "            #next_frame = torch.from_numpy(next_frame)\n",
        "            #next_frame = np.transpose(next_frame, (1, 2, 0))\n",
        "\n",
        "            #Remove the oldest item if the queue is full, in a way such that we can add a new one\n",
        "            if len(buffer)>=config.BUFFER_SIZE:\n",
        "                buffer.popleft()               #We dequeue the oldest item\n",
        "\n",
        "            #buffer.append([*state,action,reward,*next_state,done])\n",
        "            #buffer.append((state, action, reward, next_state, done))\n",
        "            buffer.append((current_frame, action_id, reward, next_frame, done))\n",
        "\n",
        "            #next_state = cv2.cvtColor(next_state, cv2.COLOR_BGR2GRAY)\n",
        "            #state = next_state    #We update the Current State\n",
        "\n",
        "            if done or truncated:\n",
        "            #if done or j==499:\n",
        "                epsilon = update_epsilon_nn(epsilon)\n",
        "                print(\"episode \", i)\n",
        "                print(\"episode cumulative reward: \", episode_reward)\n",
        "                print(\"current epsilon: \", epsilon)\n",
        "                print(\"#---------------------------------------------#\")\n",
        "                break\n",
        "\n",
        "            #Let's train the Neural Network every 4 actions and if the buffer has at least BATCH_SIZE elements\n",
        "            if((len(buffer) >= config.BATCH_SIZE) and ((j+1) % 4 == 0)):\n",
        "                batch = random.sample(buffer, config.BATCH_SIZE)\n",
        "                #dataset = np.array(batch)\n",
        "\n",
        "                #array_states = []\n",
        "                #array_targets = []\n",
        "                #array_actions = []\n",
        "\n",
        "                #print(len(batch[0][0]))\n",
        "\n",
        "                #states = torch.from_numpy((dataset[:,:8]).astype('float32'))\n",
        "                #actions = torch.from_numpy(dataset[:,8:9].astype('int64'))\n",
        "                #rewards = torch.from_numpy(dataset[:,9:10].astype('float32'))\n",
        "                #next_states = torch.from_numpy((dataset[:,10:18]).astype('float32'))\n",
        "                #dones = torch.from_numpy(dataset[:,18:19].astype('float32'))\n",
        "\n",
        "                for current_frame, action, reward, next_frame, done in batch:\n",
        "                    #states = torch.from_numpy((dataset[:,:0]).astype('uint8'))\n",
        "                    #states = dataset[0]\n",
        "                    #actions = torch.from_numpy(dataset[:,1:1].astype('float32'))\n",
        "                    #rewards = torch.from_numpy(dataset[:,2].astype('float32'))\n",
        "                    #next_states = torch.from_numpy((dataset[:,3:3]).astype('uint8'))\n",
        "                    #next_states = dataset[0][3]\n",
        "                    #dones = torch.from_numpy(dataset[:,4].astype('float32'))\n",
        "\n",
        "                    #-------vanilla dqn------------#\n",
        "\n",
        "                    \"\"\"# Find next best action so can compute the next reward for the target\n",
        "                    #predictions_next = target_model(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "                    #next_actions=np.argmax(predictions_next) # Select action with max Q-value\n",
        "\n",
        "                    #Compute corresponding (predicted) reward of next state\n",
        "                    #next_rewards = predictions_next[next_actions]\n",
        "                    next_rewards = target_model(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "                    #-------------------------------#\"\"\"\n",
        "                    #---------double dqn-------------#\n",
        "\n",
        "                    # Find next best action using model network\n",
        "                    #predictions_next = model(next_frame.type('torch.cuda.FloatTensor')).detach().numpy()\n",
        "                    #########CORREGGI DA QUI######\n",
        "                    next_frame = torch.from_numpy(next_frame.astype('float32')).to(config.DEVICE)\n",
        "                    predictions_next = model(next_frame).detach().cpu().numpy()\n",
        "                    #predictions_next = model(next_frame.type('torch.cuda.FloatTensor')).detach().cpu().numpy()\n",
        "                    #predictions_next = model(np.expand_dims(next_states, axis=0)).detach().numpy()\n",
        "                    #next_actions.append(predictions_next)\n",
        "\n",
        "                    ########################\n",
        "                    t = target_model(next_frame)\n",
        "                    predictions_next[action] = reward + config.GAMMA * max(t)\n",
        "\n",
        "                    #array_states.append(state)\n",
        "                    #array_targets.append(predictions_next)\n",
        "                    #array_actions.append(action)\n",
        "                    ########################\n",
        "\n",
        "\n",
        "                    #next_actions = np.argmax(predictions_next,axis=1) # Select action with max Q-value\n",
        "                    #array_states = np.argmax(next_actions) # Select action with max Q-value\n",
        "                    #array_states =  array_states[..., np.newaxis]\n",
        "\n",
        "\n",
        "                    #evaluate Q(s',a') founded by model using the target network\n",
        "                    #next_rewards = target_model(next_frame.type('torch.cuda.FloatTensor')).gather(1, torch.from_numpy(next_actions))\n",
        "                    #next_rewards = torch.from_numpy(evaluations[next_actions])\n",
        "\n",
        "                    #-------------------------------#\n",
        "\n",
        "                    #targets = rewards + config.GAMMA_NN*next_rewards*(1-dones)\n",
        "                    #targets = reward + config.GAMMA_NN*next_rewards*(1-done)\n",
        "\n",
        "                    #compute the predicted value of the model(output)\n",
        "                    current_frame = torch.from_numpy(current_frame.astype('float32')).to(config.DEVICE)\n",
        "                    output = model(current_frame)#.gather(1, array_actions)\n",
        "                    #compute the huber loss\n",
        "                    output =  output[..., np.newaxis]\n",
        "                    predictions_next =  predictions_next[..., np.newaxis]\n",
        "                    predictions_next = torch.from_numpy(predictions_next).to(config.DEVICE)\n",
        "                    loss = huber_loss(output, predictions_next)\n",
        "\n",
        "                    #Train network\n",
        "                    optimizer.zero_grad()#clear existing gradient\n",
        "                    loss.backward() #backpropagate the error\n",
        "                    optimizer.step() # update weights\n",
        "\n",
        "            j = j + 1\n",
        "\n",
        "\n",
        "        if (i+1) % 5 == 0:\n",
        "            #save the weight of the network\n",
        "            config.save_model(model,optimizer,i+1)\n",
        "            print(\"Save weigths in: \"+ config.CHECKPOINT_FOLDER)\n",
        "\n",
        "        if (i+1) % 5 == 0:\n",
        "            #update weights of target network every 10 actions\n",
        "            #if len(buffer) >= config.BATCH_SIZE and (j+1) % config.TARGET_FREQ_UPDATE == 0:\n",
        "            if (j+1) % config.TARGET_FREQ_UPDATE == 0:\n",
        "                print(\"Target network updated\")\n",
        "                config.load_model(config.CHECKPOINT_FOLDER,target_model,optimizer_target)\n",
        "\n",
        "\n",
        "        cum_reward_nn[i]=episode_reward\n",
        "        episode_reward = 0\n",
        "\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTI2zOGcpXiu",
        "outputId": "b479dfae-e3f9-47df-8753-c05ce7b0db30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode  0\n",
            "episode cumulative reward:  -29.577464788732573\n",
            "current epsilon:  0.99\n",
            "#---------------------------------------------#\n",
            "episode  1\n",
            "episode cumulative reward:  -24.092409240924106\n",
            "current epsilon:  0.9801\n",
            "#---------------------------------------------#\n",
            "episode  2\n",
            "episode cumulative reward:  -22.509225092250954\n",
            "current epsilon:  0.9702989999999999\n",
            "#---------------------------------------------#\n",
            "episode  3\n",
            "episode cumulative reward:  -28.3276450511947\n",
            "current epsilon:  0.96059601\n",
            "#---------------------------------------------#\n",
            "episode  4\n",
            "episode cumulative reward:  -42.24924012158096\n",
            "current epsilon:  0.9509900498999999\n",
            "#---------------------------------------------#\n",
            "Save weigths in: Checkpoints/checkpoint.pth.tar\n",
            "episode  5\n",
            "episode cumulative reward:  -35.593220338983315\n",
            "current epsilon:  0.9414801494009999\n",
            "#---------------------------------------------#\n",
            "episode  6\n",
            "episode cumulative reward:  -19.847328244274703\n",
            "current epsilon:  0.9320653479069899\n",
            "#---------------------------------------------#\n",
            "episode  7\n",
            "episode cumulative reward:  -23.954372623574255\n",
            "current epsilon:  0.92274469442792\n",
            "#---------------------------------------------#\n",
            "episode  8\n",
            "episode cumulative reward:  -31.147540983606902\n",
            "current epsilon:  0.9135172474836407\n",
            "#---------------------------------------------#\n",
            "episode  9\n",
            "episode cumulative reward:  -23.07692307692314\n",
            "current epsilon:  0.9043820750088043\n",
            "#---------------------------------------------#\n",
            "Save weigths in: Checkpoints/checkpoint.pth.tar\n",
            "episode  10\n",
            "episode cumulative reward:  -31.271477663230645\n",
            "current epsilon:  0.8953382542587163\n",
            "#---------------------------------------------#\n",
            "episode  11\n",
            "episode cumulative reward:  -39.29712460063963\n",
            "current epsilon:  0.8863848717161291\n",
            "#---------------------------------------------#\n",
            "episode  12\n",
            "episode cumulative reward:  -14.285714285714153\n",
            "current epsilon:  0.8775210229989678\n",
            "#---------------------------------------------#\n",
            "episode  13\n",
            "episode cumulative reward:  -39.87341772151961\n",
            "current epsilon:  0.8687458127689781\n",
            "#---------------------------------------------#\n",
            "episode  14\n",
            "episode cumulative reward:  -40.438871473354844\n",
            "current epsilon:  0.8600583546412883\n",
            "#---------------------------------------------#\n",
            "Save weigths in: Checkpoints/checkpoint.pth.tar\n",
            "episode  15\n",
            "episode cumulative reward:  -33.77483443708645\n",
            "current epsilon:  0.8514577710948754\n",
            "#---------------------------------------------#\n",
            "episode  16\n",
            "episode cumulative reward:  -34.70790378006926\n",
            "current epsilon:  0.8429431933839266\n",
            "#---------------------------------------------#\n",
            "episode  17\n",
            "episode cumulative reward:  -32.88590604026892\n",
            "current epsilon:  0.8345137614500874\n",
            "#---------------------------------------------#\n",
            "episode  18\n",
            "episode cumulative reward:  -41.896024464832465\n",
            "current epsilon:  0.8261686238355865\n",
            "#---------------------------------------------#\n",
            "episode  19\n",
            "episode cumulative reward:  -37.70491803278747\n",
            "current epsilon:  0.8179069375972307\n",
            "#---------------------------------------------#\n",
            "Save weigths in: Checkpoints/checkpoint.pth.tar\n",
            "episode  20\n",
            "episode cumulative reward:  -29.104477611940798\n",
            "current epsilon:  0.8097278682212583\n",
            "#---------------------------------------------#\n",
            "episode  21\n",
            "episode cumulative reward:  -44.785276073620295\n",
            "current epsilon:  0.8016305895390458\n",
            "#---------------------------------------------#\n",
            "episode  22\n",
            "episode cumulative reward:  -38.77551020408228\n",
            "current epsilon:  0.7936142836436553\n",
            "#---------------------------------------------#\n",
            "episode  23\n",
            "episode cumulative reward:  -40.97222222222288\n",
            "current epsilon:  0.7856781408072188\n",
            "#---------------------------------------------#\n",
            "episode  24\n",
            "episode cumulative reward:  -49.40476190476269\n",
            "current epsilon:  0.7778213593991465\n",
            "#---------------------------------------------#\n",
            "Save weigths in: Checkpoints/checkpoint.pth.tar\n",
            "episode  25\n",
            "episode cumulative reward:  -42.567567567568155\n",
            "current epsilon:  0.7700431458051551\n",
            "#---------------------------------------------#\n",
            "episode  26\n",
            "episode cumulative reward:  -43.8943894389446\n",
            "current epsilon:  0.7623427143471035\n",
            "#---------------------------------------------#\n",
            "episode  27\n",
            "episode cumulative reward:  -44.25087108014007\n",
            "current epsilon:  0.7547192872036325\n",
            "#---------------------------------------------#\n",
            "episode  28\n",
            "episode cumulative reward:  -42.85714285714348\n",
            "current epsilon:  0.7471720943315961\n",
            "#---------------------------------------------#\n",
            "episode  29\n",
            "episode cumulative reward:  -44.078947368421765\n",
            "current epsilon:  0.7397003733882802\n",
            "#---------------------------------------------#\n",
            "Save weigths in: Checkpoints/checkpoint.pth.tar\n",
            "episode  30\n",
            "episode cumulative reward:  -49.68553459119571\n",
            "current epsilon:  0.7323033696543974\n",
            "#---------------------------------------------#\n",
            "episode  31\n",
            "episode cumulative reward:  -41.391941391941984\n",
            "current epsilon:  0.7249803359578534\n",
            "#---------------------------------------------#\n",
            "episode  32\n",
            "episode cumulative reward:  -39.39393939393998\n",
            "current epsilon:  0.7177305325982748\n",
            "#---------------------------------------------#\n",
            "episode  33\n",
            "episode cumulative reward:  -36.1702127659579\n",
            "current epsilon:  0.7105532272722921\n",
            "#---------------------------------------------#\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCZDvbZaAKye"
      },
      "source": [
        "## Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "QUl59hUJANVv",
        "outputId": "1b59d75a-80f0-4d09-cd2b-9e73e00b1cba"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-f8c41a22e6c0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgnwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAnimation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCarRacing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCarRacing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrender_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rgb_array\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#env = CarRacing(render_mode=\"state_pixels\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#env = gym.make(\"CarRacing-v2\", domain_randomize=True, render_mode=\"state_pixels\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'CarRacing' is not defined"
          ]
        }
      ],
      "source": [
        "env = gnwrapper.Animation(CarRacing())\n",
        "env = CarRacing(render_mode=\"rgb_array\")\n",
        "#env = CarRacing(render_mode=\"state_pixels\")\n",
        "#env = gym.make(\"CarRacing-v2\", domain_randomize=True, render_mode=\"state_pixels\")\n",
        "\n",
        "env.reset()\n",
        "#env.render()\n",
        "#im = env.render()\n",
        "#im = env.render(\"state_pixels\")\n",
        "\n",
        "#plt.imshow(im)\n",
        "\n",
        "for i in range(40):\n",
        "  env.step(action=[-0.3,1,0])\n",
        "  im = env.render()\n",
        "  plt.imshow(im)\n",
        "\n",
        "\"\"\"\n",
        "def state_image_preprocess(state_image):\n",
        "    state_image = state_image.transpose((2,0,1))\n",
        "    state_image = np.ascontiguousarray(state_image, dtype=np.float32) / 255\n",
        "    state_image = torch.from_numpy(state_image)\n",
        "    return state_image.unsqueeze(0).to(device)\n",
        "\n",
        "state_image_preprocess(im).shape\n",
        "plt.imshow(state_image_preprocess(im).cpu().squeeze(0).permute(1, 2, 0).numpy())\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}