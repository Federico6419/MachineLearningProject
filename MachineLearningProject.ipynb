{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Federico6419/MachineLearningProject/blob/main/MachineLearningProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jDBR7kyywx2"
      },
      "source": [
        "## Install libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0A3otcPC9xYL",
        "outputId": "a286badf-3d77-4bb1-d001-50b68bc27f8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/953.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/953.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n",
            "Collecting swig\n",
            "  Downloading swig-4.1.1.post0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.1.1.post0\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.5.2)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.1.1.post0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2373075 sha256=be64914025948d3de5e673f6cabdb65f0cbf2649a5a1f352400ddc405733cc67\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.5\n",
            "Collecting gym-notebook-wrapper\n",
            "  Downloading gym_notebook_wrapper-1.3.3-py3-none-any.whl (9.9 kB)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (from gym-notebook-wrapper) (0.25.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from gym-notebook-wrapper) (3.7.1)\n",
            "Collecting pyvirtualdisplay (from gym-notebook-wrapper)\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from gym-notebook-wrapper) (7.34.0)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (from gym-notebook-wrapper) (1.0.3)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym->gym-notebook-wrapper) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym->gym-notebook-wrapper) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym->gym-notebook-wrapper) (0.0.8)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->gym-notebook-wrapper) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython->gym-notebook-wrapper)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->gym-notebook-wrapper) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->gym-notebook-wrapper) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->gym-notebook-wrapper) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->gym-notebook-wrapper) (3.0.39)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->gym-notebook-wrapper) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->gym-notebook-wrapper) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->gym-notebook-wrapper) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->gym-notebook-wrapper) (4.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gym-notebook-wrapper) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gym-notebook-wrapper) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gym-notebook-wrapper) (4.44.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gym-notebook-wrapper) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gym-notebook-wrapper) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gym-notebook-wrapper) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gym-notebook-wrapper) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gym-notebook-wrapper) (2.8.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy->gym-notebook-wrapper) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy->gym-notebook-wrapper) (2.31.0)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy->gym-notebook-wrapper) (0.1.10)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy->gym-notebook-wrapper) (2.31.6)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy->gym-notebook-wrapper) (0.4.9)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->gym-notebook-wrapper) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->gym-notebook-wrapper) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->gym-notebook-wrapper) (0.2.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->gym-notebook-wrapper) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->gym-notebook-wrapper) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->gym-notebook-wrapper) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->gym-notebook-wrapper) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->gym-notebook-wrapper) (2023.7.22)\n",
            "Installing collected packages: pyvirtualdisplay, jedi, gym-notebook-wrapper\n",
            "Successfully installed gym-notebook-wrapper-1.3.3 jedi-0.19.1 pyvirtualdisplay-3.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings\n",
            "  xfonts-utils xserver-common\n",
            "The following NEW packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings\n",
            "  xfonts-utils xserver-common xvfb\n",
            "0 upgraded, 9 newly installed, 0 to remove and 19 not upgraded.\n",
            "Need to get 7,814 kB of archives.\n",
            "After this operation, 11.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.2 [28.1 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.2 [864 kB]\n",
            "Fetched 7,814 kB in 3s (2,533 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 9.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "(Reading database ... 120874 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.2_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.2_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "Collecting xvfbwrapper\n",
            "  Downloading xvfbwrapper-0.2.9.tar.gz (5.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: xvfbwrapper\n",
            "  Building wheel for xvfbwrapper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for xvfbwrapper: filename=xvfbwrapper-0.2.9-py3-none-any.whl size=5008 sha256=abb61a5c10a6bb18ac16eed88511107e60d4d35833b550bde525069dba9f4c23\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/10/7d/2b7fdffccf837f7d5425931575fbee9caebe2c190931f9058b\n",
            "Successfully built xvfbwrapper\n",
            "Installing collected packages: xvfbwrapper\n",
            "Successfully installed xvfbwrapper-0.2.9\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium              #Install Gymnasium\n",
        "!pip install swig                   #This solves the error in the installation of gymnasium[box2d]\n",
        "!pip install gymnasium[box2d]       #Install Box2D\n",
        "!pip install gym-notebook-wrapper   #This installs Gym-Notebook-Wrapper, that provides small wrappers for running and rendering OpenAI Gym\n",
        "\n",
        "#To solve the xvfb missing file problem\n",
        "!sudo apt-get install xvfb\n",
        "!pip install xvfbwrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lCpyHIx0K0w"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IW5i1FfF0PeI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d2a7f89-091e-403b-bc25-687c626b2e9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MachineLearningProject'...\n",
            "remote: Enumerating objects: 344, done.\u001b[K\n",
            "remote: Counting objects: 100% (214/214), done.\u001b[K\n",
            "remote: Compressing objects: 100% (125/125), done.\u001b[K\n",
            "remote: Total 344 (delta 135), reused 139 (delta 89), pack-reused 130\u001b[K\n",
            "Receiving objects: 100% (344/344), 2.25 MiB | 29.49 MiB/s, done.\n",
            "Resolving deltas: 100% (207/207), done.\n",
            "/content/MachineLearningProject\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Federico6419/MachineLearningProject               #It clones my github repository\n",
        "%cd MachineLearningProject\n",
        "\n",
        "import gymnasium as gym\n",
        "import gnwrapper\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.utils import save_image\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import cv2\n",
        "import config\n",
        "from model import Model\n",
        "from collections import deque\n",
        "from torchvision import transforms\n",
        "import base64\n",
        "import csv\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")           #Select the Device"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "MKJKM9g2pV6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Manage Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#Initialize the Variables\n",
        "episode_reward = 0                                 #This is the Variable in which it will be stored the Reward of each Episode\n",
        "tot_negative_reward = 0                            #This Variable counts the number of consecutive Negative Rewards\n",
        "time_frame_counter = 1                             #This is the Counter of the Frames\n",
        "buffer = deque([], config.BUFFER_SIZE)             #Initialize the Queue that contains the past experience\n",
        "epsilon = config.MAX_EPSILON\n",
        "if(config.LOAD_CHECKPOINT):                        #If Checkpoints are loaded, initialize Epsilon to a specific value\n",
        "    epsilon = config.LOADED_EPSILON\n",
        "\n",
        "alpha = config.ALPHA\n",
        "decay = config.EPSILON_DECAY\n",
        "\n",
        "#Array that will store the Rewards\n",
        "cum_reward_table = np.zeros(config.NUM_EPISODES)\n",
        "cum_reward_nn = np.zeros(config.NUM_EPISODES)\n",
        "\n",
        "#Initialize the Model\n",
        "model = Model().to(config.DEVICE)\n",
        "optimizer = optim.Adam(model.parameters(), lr=config.LR)\n",
        "\n",
        "#Initialize the Target Model\n",
        "target_model = Model().to(config.DEVICE)\n",
        "optimizer_target = optim.Adam(target_model.parameters(), lr=config.LR)\n",
        "\n",
        "#If Checkpoints are loaded, load them on the Models\n",
        "if(config.LOAD_CHECKPOINT):\n",
        "    config.load_model(config.LOAD_CHECKPOINT_FOLDER,model,optimizer)\n",
        "    config.load_model(config.LOAD_CHECKPOINT_FOLDER,target_model,optimizer_target)\n",
        "\n",
        "#Initialize the Mean Squared Error Loss\n",
        "mean_squared_error = torch.nn.MSELoss()\n",
        "\n",
        "#Define the Action Space, that is composed by 12 Actions of type (Steering Wheel, Throttle, Break)\n",
        "action_space = [\n",
        "                (-1, 1, 0.2), (0, 1, 0.2), (1, 1, 0.2),\n",
        "                (-1, 1,   0), (0, 1,   0), (1, 1,   0),\n",
        "                (-1, 0, 0.2), (0, 0, 0.2), (1, 0, 0.2),\n",
        "                (-1, 0,   0), (0, 0,   0), (1, 0,   0)\n",
        "              ]\n",
        "\n",
        "\n",
        "#Here it follow the definition of the Functions that contain the Policy to choose the Action to perform\n",
        "\n",
        "#Q-Table Policy\n",
        "def select_action(state, epsilon):\n",
        "    rv = random.uniform(0, 1)\n",
        "    if rv < epsilon:\n",
        "        return action_space[random.randrange(len(action_space))]          #We sample a random action from the Action Space\n",
        "    else:\n",
        "        if state in Q_Table:\n",
        "            return action_space[np.argmax(Q_Table[state])]                #Return the action with the highest value for the current state in the Q-Table\n",
        "        else:\n",
        "            return (0, 1, 0)\n",
        "\n",
        "#Neural Network Policy\n",
        "def select_action_nn(state, epsilon):\n",
        "    rv = random.uniform(0, 1)\n",
        "    if rv < epsilon:\n",
        "        return action_space[random.randrange(len(action_space))]          #We sample a random action from the Action Space\n",
        "\n",
        "    else:\n",
        "        prediction = model(torch.from_numpy(state.astype('float32')).to(config.DEVICE)).detach().cpu().numpy()\n",
        "        action = action_space[np.argmax(prediction)]                      #Select the action with the maximum predicted value\n",
        "        return action\n",
        "\n",
        "\n",
        "#Here it follow the definition of the Functions that update the value of Epsilon\n",
        "\n",
        "#Update Epsilon in each iteration until it converges to MIN_EPSILON\n",
        "def update_epsilon(epsilon):\n",
        "    epsilon -= epsilon/100          #Let's reduce the value of Epsilon by 1/100\n",
        "    if epsilon <= config.MIN_EPSILON:\n",
        "        return config.MIN_EPSILON\n",
        "    else:\n",
        "        return epsilon\n",
        "\n",
        "#Update Epsilon in each episode multiplying it by the Decay\n",
        "def update_epsilon_nn(epsilon):\n",
        "    epsilon *= decay\n",
        "    if epsilon <= config.MIN_EPSILON:\n",
        "        return config.MIN_EPSILON\n",
        "    else:\n",
        "        return epsilon\n",
        "\n",
        "\n",
        "#This is the function that discretizes the image\n",
        "def discretize_image(image):\n",
        "    image = image.unsqueeze(0)\n",
        "\n",
        "    #Execute the 3x3 Average Pooling\n",
        "    pooling = nn.AvgPool2d(8, stride=8)\n",
        "    image = pooling(image)\n",
        "    image = image.squeeze(0)\n",
        "\n",
        "    #Create the image identifier\n",
        "    string = \"S\"\n",
        "    for i, x in enumerate(image.numpy()):\n",
        "        for j in x:\n",
        "            if(j < 30):\n",
        "                string = string + \"0\"\n",
        "            elif(j < 60):\n",
        "                string = string + \"1\"\n",
        "            elif(j < 90):\n",
        "                string = string + \"2\"\n",
        "            elif(j < 120):\n",
        "                string = string + \"3\"\n",
        "            elif(j < 150):\n",
        "                string = string + \"4\"\n",
        "            elif(j < 180):\n",
        "                string = string + \"5\"\n",
        "            elif(j < 210):\n",
        "                string = string + \"6\"\n",
        "            elif(j < 256):\n",
        "                string = string + \"7\"\n",
        "\n",
        "    return string\n",
        "\n",
        "\n",
        "#Initialize the Car Racing Environment\n",
        "env = gym.make(\"CarRacing-v2\", render_mode=\"human\")\n",
        "\n",
        "\n",
        "#Q Table\n",
        "if(config.USE_QTABLE):\n",
        "    #Initialize the Q-Table as a Dictionary\n",
        "    Q_Table = {}\n",
        "\n",
        "    for i in range(config.NUM_EPISODES):              #Iterate over the Episodes\n",
        "        state, info = env.reset()               #The state is a 96x96 Matrix\n",
        "\n",
        "        state = cv2.cvtColor(state, cv2.COLOR_BGR2GRAY)           #Convert the state into a Grayscale Image, that is a Matrix 96x96 composed by Integer values\n",
        "\n",
        "        #Discretize the image\n",
        "        state = torch.from_numpy(state.astype('float32'))\n",
        "        state = discretize_image(state)\n",
        "\n",
        "        #imgplot2 = plt.imshow(image)\n",
        "        #plt.show()\n",
        "\n",
        "        done = False\n",
        "\n",
        "        j = 0\n",
        "\n",
        "        while(True):                    #Iterate over the Steps\n",
        "            action = select_action(state, epsilon)\n",
        "            observation, reward, done, truncated, info = env.step(action)\n",
        "            observation = cv2.cvtColor(observation, cv2.COLOR_BGR2GRAY)           #Convert the state into a Grayscale Image, that is a Matrix 96x96 composed by Integer values\n",
        "            image = torch.from_numpy(observation.astype('float32'))\n",
        "            next_state = discretize_image(image)\n",
        "\n",
        "            if next_state in Q_Table:\n",
        "                next_max = np.max(Q_Table[next_state])\n",
        "            else:\n",
        "                next_max = 0\n",
        "\n",
        "            if state in Q_Table:\n",
        "                Q_Table[state][action_space.index(action)] += alpha * (reward + config.GAMMA * next_max - Q_Table[state][action_space.index(action)])\n",
        "            else:\n",
        "                Q_Table[state] = [0] * 12\n",
        "                Q_Table[state][action_space.index(action)] += alpha * (reward + config.GAMMA * next_max - Q_Table[state][action_space.index(action)])\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "            #If we obtain more than 25 consecutive negative reward, we'll terminate the current episode\n",
        "            tot_negative_reward = tot_negative_reward + 1 if time_frame_counter > 100 and reward < 0 else 0\n",
        "\n",
        "            #Increment the Reward if the current Action has the maximum value for the Throttle and the minimum value fot the Break\n",
        "            if action[1] == 1 and action[2] == 0:\n",
        "                reward *= 1.5\n",
        "\n",
        "            episode_reward += reward\n",
        "            if j % 4 == 0:\n",
        "                time_frame_counter += 1         #Update the counter of the Frames\n",
        "            j += 1\n",
        "\n",
        "            #Stop iterating if the current episode is finished, truncated, it has a negative cumulative reward or it has 25 consecutive negative rewards\n",
        "            if done or truncated or tot_negative_reward > 25 or episode_reward < 0:\n",
        "                epsilon = update_epsilon_nn(epsilon)            #Update the value of Epsilon\n",
        "                print(\"Current Episode: \", i)\n",
        "                print(\"Cumulative reward: \", episode_reward)\n",
        "                print(\"Current epsilon: \", epsilon)\n",
        "                print(\"########################### \\n\")\n",
        "                epsilon = update_epsilon(epsilon)\n",
        "                cum_reward_table[i]=episode_reward\n",
        "                episode_reward = 0      #Reset the total reward for each episode\n",
        "                tot_negative_reward = 1\n",
        "                time_frame_counter = 1\n",
        "                break\n",
        "\n",
        "\n",
        "        with open(\"test.csv\", \"w\") as outfile:\n",
        "            writer = csv.writer(outfile)      #Pass the csv file to csv.writer\n",
        "            #key_list = list(Q_Table.keys())   #Convert the dictionary keys to a list\n",
        "            writer.writerow(Q_Table.keys())\n",
        "            #Let's now iterate on each column and assign the corresponding values to the column\n",
        "            for i in range(12):\n",
        "                writer.writerow([Q_Table[x][i] for x in key_list])\n",
        "            !cp test.csv \"../drive/My Drive/\"\n",
        "\n",
        "#Use a Neural Network to approximate the Q-Function\n",
        "else:\n",
        "    for i in range(config.NUM_EPISODES):            #Iterate over the Episodes\n",
        "        state, info = env.reset()                                 #The state is a 96x96 Matrix\n",
        "        state = cv2.cvtColor(state, cv2.COLOR_BGR2GRAY)           #Convert the state into a Grayscale Image, that is a Matrix 96x96 composed by Integer values\n",
        "        #state = state.astype(float)\n",
        "        #state /= 255.0\n",
        "\n",
        "        frames_queue = deque([state]*3, maxlen = 3)               #Add the initial state into the Queue\n",
        "\n",
        "        done = False\n",
        "\n",
        "        while(True):                    #Iterate over the Steps\n",
        "\n",
        "            current_frame = np.array(frames_queue)\n",
        "\n",
        "            action = select_action_nn(current_frame, epsilon)         #Select the aAction with the maximum predicted Q-Value\n",
        "                                                                      #The Action is composed by 3 Values, that are the steering wheel, gas and breaking\n",
        "\n",
        "            rew = 0\n",
        "            #Skip 3 Frames\n",
        "            for tot in range(3):\n",
        "                next_state, reward, done, truncated, info = env.step(action)\n",
        "                rew += reward\n",
        "                if done or truncated:\n",
        "                    break\n",
        "\n",
        "            #If we obtain more than 25 consecutive negative reward, we'll terminate the current episode\n",
        "            tot_negative_reward = tot_negative_reward + 1 if time_frame_counter > 100 and reward < 0 else 0\n",
        "\n",
        "\n",
        "            #Increment the Reward if the current Action has the maximum value for the Throttle and the minimum value fot the Break\n",
        "            if action[1] == 1 and action[2] == 0:\n",
        "                rew *= 1.5\n",
        "\n",
        "            episode_reward += rew               #Update the cumulative episode reward\n",
        "\n",
        "            next_state = cv2.cvtColor(next_state, cv2.COLOR_BGR2GRAY)           #Convert the next state into a Grayscale Image\n",
        "            #Add normalization?\n",
        "\n",
        "            frames_queue.append(next_state)                                     #Append the next state to the Queue\n",
        "            next_frame = np.array(frames_queue)\n",
        "\n",
        "            #Remove the oldest item if the queue is full, in a way such that we can add a new one\n",
        "            if len(buffer)>=config.BUFFER_SIZE:\n",
        "                buffer.popleft()               #We dequeue the oldest item\n",
        "\n",
        "            buffer.append((current_frame, action_space.index(action), reward, next_frame, done))\n",
        "\n",
        "            #Stop iterating if the current episode is finished, truncated, it has a negative cumulative reward or it has 25 consecutive negative rewards\n",
        "            if done or truncated or tot_negative_reward > 25 or episode_reward < 0:\n",
        "                epsilon = update_epsilon_nn(epsilon)            #Update the value of Epsilon\n",
        "                print(\"Current Episode: \", i)\n",
        "                print(\"Cumulative reward: \", episode_reward)\n",
        "                print(\"Current epsilon: \", epsilon)\n",
        "                print(\"########################### \\n\")\n",
        "                break\n",
        "\n",
        "            #Let's train the Neural Network every 4 actions and if the buffer has at least BATCH_SIZE elements\n",
        "            if(len(buffer) >= config.BATCH_SIZE):\n",
        "                batch = random.sample(buffer, config.BATCH_SIZE)                #Shuffle randomly the elements of the Buffer\n",
        "\n",
        "                output_array = []                 #This array will contain the predictions made by the Model with respect to the current Action\n",
        "                target_array = []                 #This array will contain the Rewards updated summing the prediction made by the Target Model (Using the Bellman's Equation)\n",
        "\n",
        "                #Iterate over the shuffled elements of the Buffer\n",
        "                for current_frame, action, reward, next_frame, done in batch:\n",
        "\n",
        "                    #Use the Target Model to compute the prediction over the next frame\n",
        "                    next_frame = torch.from_numpy(next_frame.astype('float32')).to(config.DEVICE)\n",
        "                    next_frame = target_model(next_frame).detach()\n",
        "\n",
        "                    #Compute the Target using the Bellman's Equation\n",
        "                    target = reward + (1 - done) * config.GAMMA * max(next_frame)\n",
        "\n",
        "                    #Append the Target to the Target Array\n",
        "                    target_array.append(target)\n",
        "\n",
        "                    #Use the Model to compute the prediction over the current frame\n",
        "                    current_frame = torch.from_numpy(current_frame.astype('float32')).to(config.DEVICE)\n",
        "                    output = model(current_frame)\n",
        "\n",
        "                    #Append the prediction of the current Action to the Output Array\n",
        "                    output_array.append(output[action])\n",
        "\n",
        "                #Stack the two Arrays\n",
        "                output_array = torch.stack(output_array)\n",
        "                target_array = torch.stack(target_array)\n",
        "\n",
        "                #Compute the Mean Squared Error Loss\n",
        "                loss = mean_squared_error(output_array, target_array)\n",
        "\n",
        "                optimizer.zero_grad()                   #Reset the Gradients\n",
        "                loss.backward()                         #Execute the Backpropagation of the Loss\n",
        "                optimizer.step()                        #Update the weights\n",
        "\n",
        "            time_frame_counter += 1         #Update the counter of the Frames\n",
        "\n",
        "\n",
        "        #Save the weights of the Network every 5 Episodes\n",
        "        if (i+1) % 5 == 0:\n",
        "            config.save_model(model, optimizer, i+1)\n",
        "\n",
        "            #Save the Weights also on Google Drive\n",
        "            torch.save({\n",
        "                \"state_dict\": model.state_dict(),\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "            }, \"../drive/MyDrive/Checkpoint\")\n",
        "\n",
        "            print(\"Weigths saved in: \"+ config.CHECKPOINT_FOLDER)\n",
        "\n",
        "        #Update the weights of the Target Network every 5 Episodes\n",
        "        if (i+1) % 5 == 0:\n",
        "            config.load_model(config.CHECKPOINT_FOLDER, target_model, optimizer_target)\n",
        "            print(\"Target network updated\")\n",
        "\n",
        "\n",
        "        cum_reward_nn[i] = episode_reward                     #Add the current episode's reward to the array\n",
        "\n",
        "        #Reset the Variables\n",
        "        episode_reward = 0\n",
        "        tot_negative_reward = 1\n",
        "        time_frame_counter = 1\n",
        "\n",
        "\n",
        "#Close the Environment\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTI2zOGcpXiu",
        "outputId": "6844505e-2b68-41cc-b220-3810762f0f1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Current Episode:  0\n",
            "Cumulative reward:  -0.0010324483775461801\n",
            "Current epsilon:  0.99\n",
            "########################### \n",
            "\n",
            "Current Episode:  1\n",
            "Cumulative reward:  7.340559440559598\n",
            "Current epsilon:  0.9702989999999999\n",
            "########################### \n",
            "\n",
            "Current Episode:  2\n",
            "Cumulative reward:  14.519718309859218\n",
            "Current epsilon:  0.9509900498999999\n",
            "########################### \n",
            "\n",
            "Current Episode:  3\n",
            "Cumulative reward:  9.135294117647224\n",
            "Current epsilon:  0.9320653479069899\n",
            "########################### \n",
            "\n",
            "Current Episode:  4\n",
            "Cumulative reward:  32.634374999999615\n",
            "Current epsilon:  0.9135172474836408\n",
            "########################### \n",
            "\n",
            "Current Episode:  5\n",
            "Cumulative reward:  -0.049498327759063265\n",
            "Current epsilon:  0.8953382542587164\n",
            "########################### \n",
            "\n",
            "Current Episode:  6\n",
            "Cumulative reward:  2.5056105610562973\n",
            "Current epsilon:  0.877521022998968\n",
            "########################### \n",
            "\n",
            "Current Episode:  7\n",
            "Cumulative reward:  20.919798657717905\n",
            "Current epsilon:  0.8600583546412884\n",
            "########################### \n",
            "\n",
            "Current Episode:  8\n",
            "Cumulative reward:  50.908422939067535\n",
            "Current epsilon:  0.8429431933839268\n",
            "########################### \n",
            "\n",
            "Current Episode:  9\n",
            "Cumulative reward:  15.755128205128038\n",
            "Current epsilon:  0.8261686238355866\n",
            "########################### \n",
            "\n",
            "Current Episode:  10\n",
            "Cumulative reward:  2.411797752808943\n",
            "Current epsilon:  0.8097278682212584\n",
            "########################### \n",
            "\n",
            "Current Episode:  11\n",
            "Cumulative reward:  18.87404844290647\n",
            "Current epsilon:  0.7936142836436554\n",
            "########################### \n",
            "\n",
            "Current Episode:  12\n",
            "Cumulative reward:  4.175806451612939\n",
            "Current epsilon:  0.7778213593991467\n",
            "########################### \n",
            "\n",
            "Current Episode:  13\n",
            "Cumulative reward:  71.72162162162144\n",
            "Current epsilon:  0.7623427143471037\n",
            "########################### \n",
            "\n",
            "Current Episode:  14\n",
            "Cumulative reward:  5.37142857142845\n",
            "Current epsilon:  0.7471720943315963\n",
            "########################### \n",
            "\n",
            "Current Episode:  15\n",
            "Cumulative reward:  44.45604982206362\n",
            "Current epsilon:  0.7323033696543975\n",
            "########################### \n",
            "\n",
            "Current Episode:  16\n",
            "Cumulative reward:  25.14832214765112\n",
            "Current epsilon:  0.7177305325982749\n",
            "########################### \n",
            "\n",
            "Current Episode:  17\n",
            "Cumulative reward:  29.17691131498474\n",
            "Current epsilon:  0.7034476949995693\n",
            "########################### \n",
            "\n",
            "Current Episode:  18\n",
            "Cumulative reward:  30.022204472843303\n",
            "Current epsilon:  0.6894490858690778\n",
            "########################### \n",
            "\n",
            "Current Episode:  19\n",
            "Cumulative reward:  24.601374570446183\n",
            "Current epsilon:  0.6757290490602832\n",
            "########################### \n",
            "\n",
            "Current Episode:  20\n",
            "Cumulative reward:  52.53907849829321\n",
            "Current epsilon:  0.6622820409839836\n",
            "########################### \n",
            "\n",
            "Current Episode:  21\n",
            "Cumulative reward:  16.26145510835916\n",
            "Current epsilon:  0.6491026283684023\n",
            "########################### \n",
            "\n",
            "Current Episode:  22\n",
            "Cumulative reward:  49.84999999999984\n",
            "Current epsilon:  0.636185486063871\n",
            "########################### \n",
            "\n",
            "Current Episode:  23\n",
            "Cumulative reward:  42.65113268608349\n",
            "Current epsilon:  0.6235253948912001\n",
            "########################### \n",
            "\n",
            "Current Episode:  24\n",
            "Cumulative reward:  35.065853658535964\n",
            "Current epsilon:  0.6111172395328652\n",
            "########################### \n",
            "\n",
            "Current Episode:  25\n",
            "Cumulative reward:  19.5106870229002\n",
            "Current epsilon:  0.5989560064661612\n",
            "########################### \n",
            "\n",
            "Current Episode:  26\n",
            "Cumulative reward:  45.005617977527734\n",
            "Current epsilon:  0.5870367819374847\n",
            "########################### \n",
            "\n",
            "Current Episode:  27\n",
            "Cumulative reward:  16.38963210702287\n",
            "Current epsilon:  0.5753547499769287\n",
            "########################### \n",
            "\n",
            "Current Episode:  28\n",
            "Cumulative reward:  37.39765342960225\n",
            "Current epsilon:  0.5639051904523879\n",
            "########################### \n",
            "\n",
            "Current Episode:  29\n",
            "Cumulative reward:  29.577436823104026\n",
            "Current epsilon:  0.5526834771623853\n",
            "########################### \n",
            "\n",
            "Current Episode:  30\n",
            "Cumulative reward:  35.58058608058549\n",
            "Current epsilon:  0.5416850759668538\n",
            "########################### \n",
            "\n",
            "Current Episode:  31\n",
            "Cumulative reward:  66.77072243346024\n",
            "Current epsilon:  0.5309055429551134\n",
            "########################### \n",
            "\n",
            "Current Episode:  32\n",
            "Cumulative reward:  99.17084592144991\n",
            "Current epsilon:  0.5203405226503066\n",
            "########################### \n",
            "\n",
            "Current Episode:  33\n",
            "Cumulative reward:  30.58438661709965\n",
            "Current epsilon:  0.5099857462495655\n",
            "########################### \n",
            "\n",
            "Current Episode:  34\n",
            "Cumulative reward:  49.4584229390677\n",
            "Current epsilon:  0.49983702989919915\n",
            "########################### \n",
            "\n",
            "Current Episode:  35\n",
            "Cumulative reward:  25.007894736841564\n",
            "Current epsilon:  0.4898902730042051\n",
            "########################### \n",
            "\n",
            "Current Episode:  36\n",
            "Cumulative reward:  39.10517241379255\n",
            "Current epsilon:  0.48014145657142143\n",
            "########################### \n",
            "\n",
            "Current Episode:  37\n",
            "Cumulative reward:  27.070418006430153\n",
            "Current epsilon:  0.47058664158565017\n",
            "########################### \n",
            "\n",
            "Current Episode:  38\n",
            "Cumulative reward:  57.51366906474858\n",
            "Current epsilon:  0.46122196741809574\n",
            "########################### \n",
            "\n",
            "Current Episode:  39\n",
            "Cumulative reward:  41.53673469387705\n",
            "Current epsilon:  0.45204365026647564\n",
            "########################### \n",
            "\n",
            "Current Episode:  40\n",
            "Cumulative reward:  38.243939393939016\n",
            "Current epsilon:  0.44304798162617276\n",
            "########################### \n",
            "\n",
            "Current Episode:  41\n",
            "Cumulative reward:  43.37560553633156\n",
            "Current epsilon:  0.4342313267918119\n",
            "########################### \n",
            "\n",
            "Current Episode:  42\n",
            "Cumulative reward:  40.659090909090644\n",
            "Current epsilon:  0.4255901233886549\n",
            "########################### \n",
            "\n",
            "Current Episode:  43\n",
            "Cumulative reward:  148.92091254753024\n",
            "Current epsilon:  0.41712087993322067\n",
            "########################### \n",
            "\n",
            "Current Episode:  44\n",
            "Cumulative reward:  52.4641025641025\n",
            "Current epsilon:  0.4088201744225496\n",
            "########################### \n",
            "\n",
            "Current Episode:  45\n",
            "Cumulative reward:  51.839781021897686\n",
            "Current epsilon:  0.4006846529515409\n",
            "########################### \n",
            "\n",
            "Current Episode:  46\n",
            "Cumulative reward:  42.15985915492916\n",
            "Current epsilon:  0.39271102835780525\n",
            "########################### \n",
            "\n",
            "Current Episode:  47\n",
            "Cumulative reward:  47.63211920529788\n",
            "Current epsilon:  0.3848960788934849\n",
            "########################### \n",
            "\n",
            "Current Episode:  48\n",
            "Cumulative reward:  43.84459459459437\n",
            "Current epsilon:  0.37723664692350456\n",
            "########################### \n",
            "\n",
            "Current Episode:  49\n",
            "Cumulative reward:  17.48181818181753\n",
            "Current epsilon:  0.3697296376497268\n",
            "########################### \n",
            "\n",
            "Current Episode:  50\n",
            "Cumulative reward:  34.0204697986571\n",
            "Current epsilon:  0.36237201786049716\n",
            "########################### \n",
            "\n",
            "Current Episode:  51\n",
            "Cumulative reward:  58.89482758620666\n",
            "Current epsilon:  0.35516081470507327\n",
            "########################### \n",
            "\n",
            "Current Episode:  52\n",
            "Cumulative reward:  31.46188925081376\n",
            "Current epsilon:  0.3480931144924423\n",
            "########################### \n",
            "\n",
            "Current Episode:  53\n",
            "Cumulative reward:  40.72258064516109\n",
            "Current epsilon:  0.34116606151404266\n",
            "########################### \n",
            "\n",
            "Current Episode:  54\n",
            "Cumulative reward:  -89.04084967320304\n",
            "Current epsilon:  0.3343768568899132\n",
            "########################### \n",
            "\n",
            "Current Episode:  55\n",
            "Cumulative reward:  49.90211267605637\n",
            "Current epsilon:  0.32772275743780394\n",
            "########################### \n",
            "\n",
            "Current Episode:  56\n",
            "Cumulative reward:  -11.249999999999801\n",
            "Current epsilon:  0.32120107456479163\n",
            "########################### \n",
            "\n",
            "Current Episode:  57\n",
            "Cumulative reward:  79.36538461538385\n",
            "Current epsilon:  0.31480917318095225\n",
            "########################### \n",
            "\n",
            "Current Episode:  58\n",
            "Cumulative reward:  -0.09896193771621076\n",
            "Current epsilon:  0.3085444706346513\n",
            "########################### \n",
            "\n",
            "Current Episode:  59\n",
            "Cumulative reward:  56.69285714285785\n",
            "Current epsilon:  0.3024044356690217\n",
            "########################### \n",
            "\n",
            "Current Episode:  60\n",
            "Cumulative reward:  62.65206185567017\n",
            "Current epsilon:  0.29638658739920815\n",
            "########################### \n",
            "\n",
            "Current Episode:  61\n",
            "Cumulative reward:  -75.56058394160627\n",
            "Current epsilon:  0.29048849430996393\n",
            "########################### \n",
            "\n",
            "Current Episode:  62\n",
            "Cumulative reward:  70.97300380228245\n",
            "Current epsilon:  0.28470777327319563\n",
            "########################### \n",
            "\n",
            "Current Episode:  63\n",
            "Cumulative reward:  34.35372670807396\n",
            "Current epsilon:  0.279042088585059\n",
            "########################### \n",
            "\n",
            "Current Episode:  64\n",
            "Cumulative reward:  51.73907849829366\n",
            "Current epsilon:  0.2734891510222164\n",
            "########################### \n",
            "\n",
            "Current Episode:  65\n",
            "Cumulative reward:  24.907590759075074\n",
            "Current epsilon:  0.26804671691687426\n",
            "########################### \n",
            "\n",
            "Current Episode:  66\n",
            "Cumulative reward:  49.31290322580607\n",
            "Current epsilon:  0.26271258725022845\n",
            "########################### \n",
            "\n",
            "Current Episode:  67\n",
            "Cumulative reward:  46.89935064934992\n",
            "Current epsilon:  0.2574846067639489\n",
            "########################### \n",
            "\n",
            "Current Episode:  68\n",
            "Cumulative reward:  21.49832826747648\n",
            "Current epsilon:  0.25236066308934635\n",
            "########################### \n",
            "\n",
            "Current Episode:  69\n",
            "Cumulative reward:  17.706765676567095\n",
            "Current epsilon:  0.24733868589386837\n",
            "########################### \n",
            "\n",
            "Current Episode:  70\n",
            "Cumulative reward:  30.770996978851308\n",
            "Current epsilon:  0.24241664604458038\n",
            "########################### \n",
            "\n",
            "Current Episode:  71\n",
            "Cumulative reward:  46.235813148788914\n",
            "Current epsilon:  0.23759255478829325\n",
            "########################### \n",
            "\n",
            "Current Episode:  72\n",
            "Cumulative reward:  60.92785016286656\n",
            "Current epsilon:  0.2328644629480062\n",
            "########################### \n",
            "\n",
            "Current Episode:  73\n",
            "Cumulative reward:  32.567092651756646\n",
            "Current epsilon:  0.22823046013534087\n",
            "########################### \n",
            "\n",
            "Current Episode:  74\n",
            "Cumulative reward:  59.49581673306753\n",
            "Current epsilon:  0.2236886739786476\n",
            "########################### \n",
            "\n",
            "Current Episode:  75\n",
            "Cumulative reward:  43.01764705882323\n",
            "Current epsilon:  0.21923726936647253\n",
            "########################### \n",
            "\n",
            "Current Episode:  76\n",
            "Cumulative reward:  -0.007279693486919481\n",
            "Current epsilon:  0.21487444770607972\n",
            "########################### \n",
            "\n",
            "Current Episode:  77\n",
            "Cumulative reward:  36.671052631578426\n",
            "Current epsilon:  0.21059844619672874\n",
            "########################### \n",
            "\n",
            "Current Episode:  78\n",
            "Cumulative reward:  -89.39939939939993\n",
            "Current epsilon:  0.20640753711741386\n",
            "########################### \n",
            "\n",
            "Current Episode:  79\n",
            "Cumulative reward:  55.807067137808964\n",
            "Current epsilon:  0.20230002712877732\n",
            "########################### \n",
            "\n",
            "Current Episode:  80\n",
            "Cumulative reward:  56.24157303370835\n",
            "Current epsilon:  0.19827425658891465\n",
            "########################### \n",
            "\n",
            "Current Episode:  81\n",
            "Cumulative reward:  48.55990099009897\n",
            "Current epsilon:  0.19432859888279524\n",
            "########################### \n",
            "\n",
            "Current Episode:  82\n",
            "Cumulative reward:  26.03312101910759\n",
            "Current epsilon:  0.19046145976502762\n",
            "########################### \n",
            "\n",
            "Current Episode:  83\n",
            "Cumulative reward:  62.532899628253716\n",
            "Current epsilon:  0.18667127671570355\n",
            "########################### \n",
            "\n",
            "Current Episode:  84\n",
            "Cumulative reward:  45.56993127147745\n",
            "Current epsilon:  0.18295651830906107\n",
            "########################### \n",
            "\n",
            "Current Episode:  85\n",
            "Cumulative reward:  76.98036437247103\n",
            "Current epsilon:  0.17931568359471076\n",
            "########################### \n",
            "\n",
            "Current Episode:  86\n",
            "Cumulative reward:  -7.603654485048992\n",
            "Current epsilon:  0.175747301491176\n",
            "########################### \n",
            "\n",
            "Current Episode:  87\n",
            "Cumulative reward:  -68.43275862069012\n",
            "Current epsilon:  0.17224993019150162\n",
            "########################### \n",
            "\n",
            "Current Episode:  88\n",
            "Cumulative reward:  81.3967213114768\n",
            "Current epsilon:  0.16882215658069075\n",
            "########################### \n",
            "\n",
            "Current Episode:  89\n",
            "Cumulative reward:  49.70211267605646\n",
            "Current epsilon:  0.16546259566473498\n",
            "########################### \n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}