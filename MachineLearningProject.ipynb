{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Federico6419/MachineLearningProject/blob/main/MachineLearningProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jDBR7kyywx2"
      },
      "source": [
        "## Install libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0A3otcPC9xYL",
        "outputId": "81d0142e-9634-4697-9705-231d93634a23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/953.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/953.9 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n",
            "Collecting swig\n",
            "  Downloading swig-4.1.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.1.1\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.5.2)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.1.1)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2373075 sha256=4623dee892aee4a16e698b2345a51a27d638358642c7fe08fa2423cb326c7ec1\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.5\n",
            "Collecting gym-notebook-wrapper\n",
            "  Downloading gym_notebook_wrapper-1.3.3-py3-none-any.whl (9.9 kB)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (from gym-notebook-wrapper) (0.25.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from gym-notebook-wrapper) (3.7.1)\n",
            "Collecting pyvirtualdisplay (from gym-notebook-wrapper)\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from gym-notebook-wrapper) (7.34.0)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (from gym-notebook-wrapper) (1.0.3)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym->gym-notebook-wrapper) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym->gym-notebook-wrapper) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym->gym-notebook-wrapper) (0.0.8)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->gym-notebook-wrapper) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython->gym-notebook-wrapper)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->gym-notebook-wrapper) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->gym-notebook-wrapper) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->gym-notebook-wrapper) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->gym-notebook-wrapper) (3.0.39)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->gym-notebook-wrapper) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->gym-notebook-wrapper) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->gym-notebook-wrapper) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->gym-notebook-wrapper) (4.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gym-notebook-wrapper) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gym-notebook-wrapper) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gym-notebook-wrapper) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gym-notebook-wrapper) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gym-notebook-wrapper) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gym-notebook-wrapper) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gym-notebook-wrapper) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gym-notebook-wrapper) (2.8.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy->gym-notebook-wrapper) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy->gym-notebook-wrapper) (2.31.0)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy->gym-notebook-wrapper) (0.1.10)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy->gym-notebook-wrapper) (2.31.5)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy->gym-notebook-wrapper) (0.4.9)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->gym-notebook-wrapper) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->gym-notebook-wrapper) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->gym-notebook-wrapper) (0.2.8)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->gym-notebook-wrapper) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->gym-notebook-wrapper) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->gym-notebook-wrapper) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->gym-notebook-wrapper) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->gym-notebook-wrapper) (2023.7.22)\n",
            "Installing collected packages: pyvirtualdisplay, jedi, gym-notebook-wrapper\n",
            "Successfully installed gym-notebook-wrapper-1.3.3 jedi-0.19.1 pyvirtualdisplay-3.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings\n",
            "  xfonts-utils xserver-common\n",
            "The following NEW packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings\n",
            "  xfonts-utils xserver-common xvfb\n",
            "0 upgraded, 9 newly installed, 0 to remove and 18 not upgraded.\n",
            "Need to get 7,812 kB of archives.\n",
            "After this operation, 11.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.1 [28.0 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.1 [863 kB]\n",
            "Fetched 7,812 kB in 2s (4,223 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 9.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "(Reading database ... 120874 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.1_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.1) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.1_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.1) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.1) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "Collecting xvfbwrapper\n",
            "  Downloading xvfbwrapper-0.2.9.tar.gz (5.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: xvfbwrapper\n",
            "  Building wheel for xvfbwrapper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for xvfbwrapper: filename=xvfbwrapper-0.2.9-py3-none-any.whl size=5009 sha256=079cc8fc6efba59604b6193104d31040510a79d33dcafa7e1fdafb48469a59c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/10/7d/2b7fdffccf837f7d5425931575fbee9caebe2c190931f9058b\n",
            "Successfully built xvfbwrapper\n",
            "Installing collected packages: xvfbwrapper\n",
            "Successfully installed xvfbwrapper-0.2.9\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n",
        "!pip install swig     #This solves the errori in the installation of gymnasium[box2d]\n",
        "!pip install gymnasium[box2d]\n",
        "!pip install gym-notebook-wrapper   #This installs Gym-Notebook-Wrapper, that provides small wrappers for running and rendering OpenAI Gym\n",
        "\n",
        "#To solve the xvfb missing file problem\n",
        "!sudo apt-get install xvfb\n",
        "!pip install xvfbwrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lCpyHIx0K0w"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IW5i1FfF0PeI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1e05bf4-8a55-496b-ee90-e99085c7fd59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MachineLearningProject'...\n",
            "remote: Enumerating objects: 184, done.\u001b[K\n",
            "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 184 (delta 34), reused 4 (delta 4), pack-reused 130\u001b[K\n",
            "Receiving objects: 100% (184/184), 118.20 KiB | 896.00 KiB/s, done.\n",
            "Resolving deltas: 100% (106/106), done.\n",
            "/content/MachineLearningProject\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Federico6419/MachineLearningProject          #It clones my github repository\n",
        "%cd MachineLearningProject\n",
        "\n",
        "import gymnasium as gym\n",
        "import gnwrapper\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "import cv2\n",
        "\n",
        "import config\n",
        "from model import Model\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# F"
      ],
      "metadata": {
        "id": "MKJKM9g2pV6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "episode_reward = 0\n",
        "tot_negative_reward = 1\n",
        "time_frame_counter = 1\n",
        "#Add negative reward?\n",
        "buffer = deque([], config.BUFFER_SIZE)             #Initialize the Queue that contains the past experience\n",
        "epsilon = config.MAX_EPSILON\n",
        "alpha = config.ALPHA\n",
        "decay = config.EPSILON_DECAY\n",
        "\n",
        "#For the plotting\n",
        "cum_reward_table = np.zeros(config.NUM_EPISODES)\n",
        "cum_reward_nn = np.zeros(config.NUM_EPISODES)\n",
        "\n",
        "#Initialize the Model\n",
        "model = Model().to(config.DEVICE)\n",
        "\n",
        "#Initialize the Target Model\n",
        "target_model = Model().to(config.DEVICE)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=config.LR)\n",
        "optimizer_target = optim.Adam(target_model.parameters(), lr=config.LR)\n",
        "\n",
        "#huber_loss=nn.HuberLoss(delta=1.0)\n",
        "mean_squared_error = torch.nn.MSELoss()\n",
        "\n",
        "action_space = [\n",
        "                (-1, 1, 0.2), (0, 1, 0.2), (1, 1, 0.2),\n",
        "                (-1, 1,   0), (0, 1,   0), (1, 1,   0),               #(Steering Wheel, Gas, Break)\n",
        "                (-1, 0, 0.2), (0, 0, 0.2), (1, 0, 0.2),               #Range -1~1 0~1 0~1\n",
        "                (-1, 0,   0), (0, 0,   0), (1, 0,   0)\n",
        "              ]\n",
        "\n",
        "#Define the policy to know how chose the action\n",
        "#Q-Table\n",
        "def select_action(state, epsilon):\n",
        "    rv = random.uniform(0, 1)\n",
        "    if rv < epsilon:\n",
        "        return env.action_space.sample()\n",
        "    else:\n",
        "        return np.argmax(Q[state])\n",
        "\n",
        "#Neural Network\n",
        "def select_action_nn(state, epsilon):\n",
        "    rv = random.uniform(0, 1)\n",
        "    if rv < epsilon:\n",
        "        return action_space[random.randrange(len(action_space))]          #We sample a random action\n",
        "\n",
        "    else:\n",
        "        prediction = model(torch.from_numpy(state.astype('float32')).to(config.DEVICE)).detach().cpu().numpy()\n",
        "        action = action_space[np.argmax(prediction)]              #Select the action with the maximum predicted Q-Value\n",
        "\n",
        "        return action\n",
        "\n",
        "\n",
        "## update the epsilon value along the iteration until converges to MIN_EPSILON\n",
        "def update_epsilon(epsilon):\n",
        "    epsilon -= epsilon/100 # reduce epsilon by 1/100\n",
        "    if epsilon<=config.MIN_EPSILON:\n",
        "        return config.MIN_EPSILON\n",
        "    else:\n",
        "        return epsilon\n",
        "\n",
        "## update the epsilon every episode by epsilon decay variable\n",
        "def update_epsilon_nn(epsilon):\n",
        "    epsilon *= decay\n",
        "    if epsilon<=config.MIN_EPSILON:\n",
        "        return config.MIN_EPSILON\n",
        "    else:\n",
        "        return epsilon\n",
        "\n",
        "\n",
        "#env = gym.make(\"CarRacing-v2\", render_mode=\"human\")\n",
        "env = gym.make(\"CarRacing-v2\", render_mode=\"rgb_array\")\n",
        "\n",
        "\n",
        "if(config.USE_QTABLE):\n",
        "    # define the Q table\n",
        "    #Q = np.zeros([27684, env.action_space.n]) # little discretization\n",
        "    Q = np.zeros([19051200, env.action_space.n]) #big discretization\n",
        "\n",
        "###see the limit of the values of the box observation space\n",
        "#print(env.observation_space.high)\n",
        "#print(env.observation_space.low)\n",
        "\n",
        "###see in more detail the action space and the observation space\n",
        "#print(env.action_space)\n",
        "#print(env.observation_space)\n",
        "\n",
        "\n",
        "if(config.USE_QTABLE): # use a q table to reach the goal\n",
        "    for i in range(config.NUM_EPISODES):\n",
        "        observation, info = env.reset()# use seed to have same initial state\n",
        "        #state = config.discretize(observation)\n",
        "        state = config.big_discretize(observation)\n",
        "\n",
        "        for j in range(500):\n",
        "            action = select_action(state,epsilon)\n",
        "            obv, reward, done, truncated, info = env.step(action)\n",
        "            #next_state = config.discretize(obv)\n",
        "            next_state = config.big_discretize(obv)\n",
        "\n",
        "            next_max = np.max(Q[next_state])\n",
        "\n",
        "            Q[state,action] += alpha*(reward+config.GAMMA*next_max-Q[state,action])\n",
        "            state = next_state\n",
        "\n",
        "            episode_reward += reward\n",
        "\n",
        "            if done or truncated:\n",
        "                break\n",
        "\n",
        "        print(\"episode: \", i)\n",
        "        print(\"episode cumulative reward : \", episode_reward)\n",
        "        print(\"epsilon: \",epsilon)\n",
        "        epsilon = update_epsilon(epsilon)\n",
        "        cum_reward_table[i]=episode_reward\n",
        "        episode_reward = 0 #reset the total reward each episode\n",
        "\n",
        "    #save the q table for testing\n",
        "    #np.savetxt('q_table.csv', Q, delimiter=','fmt='%f18')\n",
        "    #np.savetxt('q_table_little_discretization2000.csv', Q, delimiter=',') # full precision\n",
        "    np.savetxt('q_table_big_discretization1000.csv', Q, delimiter=',') # full precision\n",
        "\n",
        "else:             #Use a Neural Network to approximate the Q Function\n",
        "    for i in range(config.NUM_EPISODES):\n",
        "        state, info = env.reset()               #The state is a 96x96 Matrix, that contains elements composed by 3 Colours RGB\n",
        "        #state = state.astype(float)\n",
        "        #state /= 255.0\n",
        "        state = cv2.cvtColor(state, cv2.COLOR_BGR2GRAY)           #Convert the state into a Grayscale Image, that is a Matrix 96x96 composed by Integer values\n",
        "        #Add normalization?\n",
        "\n",
        "        frames_queue = deque([state]*3, maxlen = 3)\n",
        "\n",
        "        done = False\n",
        "\n",
        "        #for j in range(500):\n",
        "        j = 0\n",
        "        while(True):\n",
        "\n",
        "            #ATTENZIONE, NON SO SE CONVERTIRE IN NP ARRAY E POI IN TENSOR SIA GIUSTO\n",
        "            current_frame = np.array(frames_queue)\n",
        "            #current_frame = torch.from_numpy(current_frame)\n",
        "            #current_frame = np.transpose(current_frame, (1, 2, 0))\n",
        "\n",
        "            #action = select_action_nn(state, epsilon)\n",
        "            action = select_action_nn(current_frame, epsilon)                      #The Action is composed by 3 Values, that are the steering, gas and breaking\n",
        "\n",
        "            rew = 0\n",
        "            #Skip Frames\n",
        "            for tot in range(3):\n",
        "                next_state, reward, done, truncated, info = env.step(action)\n",
        "                rew += reward\n",
        "                if done or truncated:\n",
        "                    break\n",
        "\n",
        "            # If continually getting negative reward 10 times after the tolerance steps, terminate this episode\n",
        "            tot_negative_reward = tot_negative_reward + 1 if time_frame_counter > 100 and reward < 0 else 0\n",
        "\n",
        "\n",
        "            # Extra bonus for the model if it uses full gas\n",
        "            #if action[1] == 1 and action[2] == 0:\n",
        "            #    rew *= 1.5\n",
        "\n",
        "            episode_reward += rew\n",
        "\n",
        "            next_state = cv2.cvtColor(next_state, cv2.COLOR_BGR2GRAY)\n",
        "            #Add normalization?\n",
        "\n",
        "            #ATTENZIONE, NON SO SE CONVERTIRE IN NP ARRAY E POI IN TENSOR SIA GIUSTO\n",
        "            frames_queue.append(next_state)\n",
        "            next_frame = np.array(frames_queue)\n",
        "            #next_frame = torch.from_numpy(next_frame)\n",
        "            #next_frame = np.transpose(next_frame, (1, 2, 0))\n",
        "\n",
        "            #Remove the oldest item if the queue is full, in a way such that we can add a new one\n",
        "            if len(buffer)>=config.BUFFER_SIZE:\n",
        "                buffer.popleft()               #We dequeue the oldest item\n",
        "\n",
        "            #buffer.append([*state,action,reward,*next_state,done])\n",
        "            #buffer.append((state, action, reward, next_state, done))\n",
        "            buffer.append((current_frame, action_space.index(action), reward, next_frame, done))\n",
        "\n",
        "            #next_state = cv2.cvtColor(next_state, cv2.COLOR_BGR2GRAY)\n",
        "            #state = next_state    #We update the Current State\n",
        "\n",
        "            #if done or truncated or tot_negative_reward > 25 or episode_reward < 0:\n",
        "            if done or truncated:\n",
        "            #if done or j==499:\n",
        "                epsilon = update_epsilon_nn(epsilon)\n",
        "                print(\"episode \", i)\n",
        "                print(\"episode cumulative reward: \", episode_reward)\n",
        "                print(\"current epsilon: \", epsilon)\n",
        "                print(\"#---------------------------------------------#\")\n",
        "                break\n",
        "\n",
        "            #Let's train the Neural Network every 4 actions and if the buffer has at least BATCH_SIZE elements\n",
        "            #if((len(buffer) >= config.BATCH_SIZE) and ((j+1) % 4 == 0)):\n",
        "            if(len(buffer) >= config.BATCH_SIZE):\n",
        "                batch = random.sample(buffer, config.BATCH_SIZE)\n",
        "                #dataset = np.array(batch)\n",
        "\n",
        "                #array_states = []\n",
        "                #array_targets = []\n",
        "                #array_actions = []\n",
        "\n",
        "                #print(len(batch[0][0]))\n",
        "\n",
        "                #states = torch.from_numpy((dataset[:,:8]).astype('float32'))\n",
        "                #actions = torch.from_numpy(dataset[:,8:9].astype('int64'))\n",
        "                #rewards = torch.from_numpy(dataset[:,9:10].astype('float32'))\n",
        "                #next_states = torch.from_numpy((dataset[:,10:18]).astype('float32'))\n",
        "                #dones = torch.from_numpy(dataset[:,18:19].astype('float32'))\n",
        "\n",
        "                for current_frame, action, reward, next_frame, done in batch:\n",
        "                    #states = torch.from_numpy((dataset[:,:0]).astype('uint8'))\n",
        "                    #states = dataset[0]\n",
        "                    #actions = torch.from_numpy(dataset[:,1:1].astype('float32'))\n",
        "                    #rewards = torch.from_numpy(dataset[:,2].astype('float32'))\n",
        "                    #next_states = torch.from_numpy((dataset[:,3:3]).astype('uint8'))\n",
        "                    #next_states = dataset[0][3]\n",
        "                    #dones = torch.from_numpy(dataset[:,4].astype('float32'))\n",
        "\n",
        "                    #-------vanilla dqn------------#\n",
        "\n",
        "                    \"\"\"# Find next best action so can compute the next reward for the target\n",
        "                    #predictions_next = target_model(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "                    #next_actions=np.argmax(predictions_next) # Select action with max Q-value\n",
        "\n",
        "                    #Compute corresponding (predicted) reward of next state\n",
        "                    #next_rewards = predictions_next[next_actions]\n",
        "                    next_rewards = target_model(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "                    #-------------------------------#\"\"\"\n",
        "                    #---------double dqn-------------#\n",
        "\n",
        "                    # Find next best action using model network\n",
        "                    next_frame = torch.from_numpy(next_frame.astype('float32')).to(config.DEVICE)\n",
        "                    predictions_next = model(next_frame).detach().cpu().numpy()\n",
        "\n",
        "                    ########################\n",
        "                    t = target_model(next_frame)\n",
        "                    predictions_next[action] = reward + config.GAMMA * max(t)\n",
        "\n",
        "                    #array_states.append(state)\n",
        "                    #array_targets.append(predictions_next)\n",
        "                    #array_actions.append(action)\n",
        "                    ########################\n",
        "\n",
        "\n",
        "                    #next_actions = np.argmax(predictions_next,axis=1) # Select action with max Q-value\n",
        "                    #array_states = np.argmax(next_actions) # Select action with max Q-value\n",
        "                    #array_states =  array_states[..., np.newaxis]\n",
        "\n",
        "\n",
        "                    #evaluate Q(s',a') founded by model using the target network\n",
        "                    #next_rewards = target_model(next_frame.type('torch.cuda.FloatTensor')).gather(1, torch.from_numpy(next_actions))\n",
        "                    #next_rewards = torch.from_numpy(evaluations[next_actions])\n",
        "\n",
        "                    #-------------------------------#\n",
        "\n",
        "                    #targets = rewards + config.GAMMA_NN*next_rewards*(1-dones)\n",
        "                    #targets = reward + config.GAMMA_NN*next_rewards*(1-done)\n",
        "\n",
        "\n",
        "                    #compute the predicted value of the model(output)\n",
        "                    current_frame = torch.from_numpy(current_frame.astype('float32')).to(config.DEVICE)\n",
        "                    output = model(current_frame)#.gather(1, array_actions)\n",
        "                    #compute the huber loss\n",
        "                    output =  output[..., np.newaxis]\n",
        "                    predictions_next =  predictions_next[..., np.newaxis]\n",
        "                    predictions_next = torch.from_numpy(predictions_next).to(config.DEVICE)\n",
        "                    #loss = huber_loss(output, predictions_next)\n",
        "                    loss = mean_squared_error(output, predictions_next)\n",
        "\n",
        "\n",
        "                    #Train network\n",
        "                    optimizer.zero_grad()#clear existing gradient\n",
        "                    loss.backward() #backpropagate the error\n",
        "                    optimizer.step() # update weights\n",
        "\n",
        "            j = j + 1\n",
        "            time_frame_counter += 1\n",
        "\n",
        "\n",
        "        if (i+1) % 5 == 0:\n",
        "            #save the weight of the network\n",
        "            config.save_model(model,optimizer,i+1)\n",
        "            print(\"Save weigths in: \"+ config.CHECKPOINT_FOLDER)\n",
        "\n",
        "        if (i+1) % 5 == 0:\n",
        "            #update weights of target network every 10 actions\n",
        "            #if len(buffer) >= config.BATCH_SIZE and (j+1) % config.TARGET_FREQ_UPDATE == 0:\n",
        "            #if (j+1) % config.TARGET_FREQ_UPDATE == 0:\n",
        "                #print(\"Target network updated\")\n",
        "                #config.load_model(config.CHECKPOINT_FOLDER,target_model,optimizer_target)\n",
        "            print(\"Target network updated\")\n",
        "            config.load_model(config.CHECKPOINT_FOLDER,target_model,optimizer_target)\n",
        "\n",
        "\n",
        "\n",
        "        cum_reward_nn[i]=episode_reward\n",
        "        episode_reward = 0\n",
        "        tot_negative_reward = 1\n",
        "        time_frame_counter = 1\n",
        "\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTI2zOGcpXiu",
        "outputId": "b68b46a9-2a1a-4878-88f1-a11157c10a5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode  0\n",
            "episode cumulative reward:  -43.6619718309867\n",
            "current epsilon:  0.99\n",
            "#---------------------------------------------#\n",
            "episode  1\n",
            "episode cumulative reward:  -53.17725752508424\n",
            "current epsilon:  0.9801\n",
            "#---------------------------------------------#\n",
            "episode  2\n",
            "episode cumulative reward:  8.97435897435784\n",
            "current epsilon:  0.9702989999999999\n",
            "#---------------------------------------------#\n",
            "episode  3\n",
            "episode cumulative reward:  -39.59731543624191\n",
            "current epsilon:  0.96059601\n",
            "#---------------------------------------------#\n",
            "episode  4\n",
            "episode cumulative reward:  -51.951951951952516\n",
            "current epsilon:  0.9509900498999999\n",
            "#---------------------------------------------#\n",
            "Save weigths in: Checkpoints/checkpoint.pth.tar\n",
            "Target network updated\n",
            "episode  5\n",
            "episode cumulative reward:  -121.99506172839504\n",
            "current epsilon:  0.9414801494009999\n",
            "#---------------------------------------------#\n",
            "episode  6\n",
            "episode cumulative reward:  -113.1245614035092\n",
            "current epsilon:  0.9320653479069899\n",
            "#---------------------------------------------#\n",
            "episode  7\n",
            "episode cumulative reward:  -150.2886287625422\n",
            "current epsilon:  0.92274469442792\n",
            "#---------------------------------------------#\n",
            "episode  8\n",
            "episode cumulative reward:  -60.00000000000068\n",
            "current epsilon:  0.9135172474836407\n",
            "#---------------------------------------------#\n",
            "episode  9\n",
            "episode cumulative reward:  -5.30303030303128\n",
            "current epsilon:  0.9043820750088043\n",
            "#---------------------------------------------#\n",
            "Save weigths in: Checkpoints/checkpoint.pth.tar\n",
            "Target network updated\n",
            "episode  10\n",
            "episode cumulative reward:  -68.99224806201586\n",
            "current epsilon:  0.8953382542587163\n",
            "#---------------------------------------------#\n",
            "episode  11\n",
            "episode cumulative reward:  -60.912052117264345\n",
            "current epsilon:  0.8863848717161291\n",
            "#---------------------------------------------#\n",
            "episode  12\n",
            "episode cumulative reward:  -32.330827067669816\n",
            "current epsilon:  0.8775210229989678\n",
            "#---------------------------------------------#\n",
            "episode  13\n",
            "episode cumulative reward:  -34.48275862069024\n",
            "current epsilon:  0.8687458127689781\n",
            "#---------------------------------------------#\n",
            "episode  14\n",
            "episode cumulative reward:  -74.82014388489205\n",
            "current epsilon:  0.8600583546412883\n",
            "#---------------------------------------------#\n",
            "Save weigths in: Checkpoints/checkpoint.pth.tar\n",
            "Target network updated\n",
            "episode  15\n",
            "episode cumulative reward:  -50.35460992907865\n",
            "current epsilon:  0.8514577710948754\n",
            "#---------------------------------------------#\n",
            "episode  16\n",
            "episode cumulative reward:  -59.55882352941233\n",
            "current epsilon:  0.8429431933839266\n",
            "#---------------------------------------------#\n",
            "episode  17\n",
            "episode cumulative reward:  -24.398625429553793\n",
            "current epsilon:  0.8345137614500874\n",
            "#---------------------------------------------#\n",
            "episode  18\n",
            "episode cumulative reward:  -38.181818181818684\n",
            "current epsilon:  0.8261686238355865\n",
            "#---------------------------------------------#\n",
            "episode  19\n",
            "episode cumulative reward:  -61.86440677966178\n",
            "current epsilon:  0.8179069375972307\n",
            "#---------------------------------------------#\n",
            "Save weigths in: Checkpoints/checkpoint.pth.tar\n",
            "Target network updated\n",
            "episode  20\n",
            "episode cumulative reward:  -66.77740863787425\n",
            "current epsilon:  0.8097278682212583\n",
            "#---------------------------------------------#\n",
            "episode  21\n",
            "episode cumulative reward:  -24.137931034483078\n",
            "current epsilon:  0.8016305895390458\n",
            "#---------------------------------------------#\n",
            "episode  22\n",
            "episode cumulative reward:  -65.8703071672361\n",
            "current epsilon:  0.7936142836436553\n",
            "#---------------------------------------------#\n",
            "episode  23\n",
            "episode cumulative reward:  -50.00000000000065\n",
            "current epsilon:  0.7856781408072188\n",
            "#---------------------------------------------#\n",
            "episode  24\n",
            "episode cumulative reward:  -39.71631205673796\n",
            "current epsilon:  0.7778213593991465\n",
            "#---------------------------------------------#\n",
            "Save weigths in: Checkpoints/checkpoint.pth.tar\n",
            "Target network updated\n",
            "episode  25\n",
            "episode cumulative reward:  -2.7237354085612817\n",
            "current epsilon:  0.7700431458051551\n",
            "#---------------------------------------------#\n",
            "episode  26\n",
            "episode cumulative reward:  -150.76086956521777\n",
            "current epsilon:  0.7623427143471035\n",
            "#---------------------------------------------#\n",
            "episode  27\n",
            "episode cumulative reward:  -153.6658682634735\n",
            "current epsilon:  0.7547192872036325\n",
            "#---------------------------------------------#\n",
            "episode  28\n",
            "episode cumulative reward:  -126.80555555555574\n",
            "current epsilon:  0.7471720943315961\n",
            "#---------------------------------------------#\n",
            "episode  29\n",
            "episode cumulative reward:  -78.26086956521716\n",
            "current epsilon:  0.7397003733882802\n",
            "#---------------------------------------------#\n",
            "Save weigths in: Checkpoints/checkpoint.pth.tar\n",
            "Target network updated\n",
            "episode  30\n",
            "episode cumulative reward:  -63.9344262295089\n",
            "current epsilon:  0.7323033696543974\n",
            "#---------------------------------------------#\n",
            "episode  31\n",
            "episode cumulative reward:  -61.67247386759631\n",
            "current epsilon:  0.7249803359578534\n",
            "#---------------------------------------------#\n",
            "episode  32\n",
            "episode cumulative reward:  -12.650602409638807\n",
            "current epsilon:  0.7177305325982748\n",
            "#---------------------------------------------#\n",
            "episode  33\n",
            "episode cumulative reward:  -39.92932862190881\n",
            "current epsilon:  0.7105532272722921\n",
            "#---------------------------------------------#\n",
            "episode  34\n",
            "episode cumulative reward:  -160.02414860681168\n",
            "current epsilon:  0.7034476949995692\n",
            "#---------------------------------------------#\n",
            "Save weigths in: Checkpoints/checkpoint.pth.tar\n",
            "Target network updated\n",
            "episode  35\n",
            "episode cumulative reward:  -56.521739130435456\n",
            "current epsilon:  0.6964132180495735\n",
            "#---------------------------------------------#\n",
            "episode  36\n",
            "episode cumulative reward:  -83.27759197324364\n",
            "current epsilon:  0.6894490858690777\n",
            "#---------------------------------------------#\n",
            "episode  37\n",
            "episode cumulative reward:  -76.82119205297998\n",
            "current epsilon:  0.682554595010387\n",
            "#---------------------------------------------#\n",
            "episode  38\n",
            "episode cumulative reward:  -74.11003236245953\n",
            "current epsilon:  0.6757290490602831\n",
            "#---------------------------------------------#\n",
            "episode  39\n",
            "episode cumulative reward:  -141.64600638977672\n",
            "current epsilon:  0.6689717585696803\n",
            "#---------------------------------------------#\n",
            "Save weigths in: Checkpoints/checkpoint.pth.tar\n",
            "Target network updated\n",
            "episode  40\n",
            "episode cumulative reward:  -73.18007662835264\n",
            "current epsilon:  0.6622820409839835\n",
            "#---------------------------------------------#\n",
            "episode  41\n",
            "episode cumulative reward:  -38.62815884476563\n",
            "current epsilon:  0.6556592205741436\n",
            "#---------------------------------------------#\n",
            "episode  42\n",
            "episode cumulative reward:  -52.861952861953235\n",
            "current epsilon:  0.6491026283684022\n",
            "#---------------------------------------------#\n",
            "episode  43\n",
            "episode cumulative reward:  -136.88231292517034\n",
            "current epsilon:  0.6426116020847181\n",
            "#---------------------------------------------#\n",
            "episode  44\n",
            "episode cumulative reward:  -79.7979797979795\n",
            "current epsilon:  0.6361854860638709\n",
            "#---------------------------------------------#\n",
            "Save weigths in: Checkpoints/checkpoint.pth.tar\n",
            "Target network updated\n",
            "episode  45\n",
            "episode cumulative reward:  -56.38629283489147\n",
            "current epsilon:  0.6298236312032323\n",
            "#---------------------------------------------#\n",
            "episode  46\n",
            "episode cumulative reward:  -72.50859106529181\n",
            "current epsilon:  0.6235253948912\n",
            "#---------------------------------------------#\n",
            "episode  47\n",
            "episode cumulative reward:  -48.45360824742338\n",
            "current epsilon:  0.617290140942288\n",
            "#---------------------------------------------#\n",
            "episode  48\n",
            "episode cumulative reward:  -56.22895622895688\n",
            "current epsilon:  0.6111172395328651\n",
            "#---------------------------------------------#\n",
            "episode  49\n",
            "episode cumulative reward:  -63.57615894039812\n",
            "current epsilon:  0.6050060671375365\n",
            "#---------------------------------------------#\n",
            "Save weigths in: Checkpoints/checkpoint.pth.tar\n",
            "Target network updated\n",
            "episode  50\n",
            "episode cumulative reward:  0.719424460430729\n",
            "current epsilon:  0.5989560064661611\n",
            "#---------------------------------------------#\n",
            "episode  51\n",
            "episode cumulative reward:  -57.14285714285791\n",
            "current epsilon:  0.5929664464014994\n",
            "#---------------------------------------------#\n",
            "episode  52\n",
            "episode cumulative reward:  -67.0329670329674\n",
            "current epsilon:  0.5870367819374844\n",
            "#---------------------------------------------#\n",
            "episode  53\n",
            "episode cumulative reward:  -44.281524926686835\n",
            "current epsilon:  0.5811664141181095\n",
            "#---------------------------------------------#\n",
            "episode  54\n",
            "episode cumulative reward:  -38.62815884476613\n",
            "current epsilon:  0.5753547499769285\n",
            "#---------------------------------------------#\n",
            "Save weigths in: Checkpoints/checkpoint.pth.tar\n",
            "Target network updated\n",
            "episode  55\n",
            "episode cumulative reward:  -137.5627450980397\n",
            "current epsilon:  0.5696012024771592\n",
            "#---------------------------------------------#\n",
            "episode  56\n",
            "episode cumulative reward:  -73.9776951672863\n",
            "current epsilon:  0.5639051904523876\n",
            "#---------------------------------------------#\n",
            "episode  57\n",
            "episode cumulative reward:  -24.13793103448328\n",
            "current epsilon:  0.5582661385478638\n",
            "#---------------------------------------------#\n",
            "episode  58\n",
            "episode cumulative reward:  -61.41479099678523\n",
            "current epsilon:  0.5526834771623851\n",
            "#---------------------------------------------#\n",
            "episode  59\n",
            "episode cumulative reward:  -58.01526717557333\n",
            "current epsilon:  0.5471566423907612\n",
            "#---------------------------------------------#\n",
            "Save weigths in: Checkpoints/checkpoint.pth.tar\n",
            "Target network updated\n",
            "episode  60\n",
            "episode cumulative reward:  -71.1538461538463\n",
            "current epsilon:  0.5416850759668536\n",
            "#---------------------------------------------#\n",
            "episode  61\n",
            "episode cumulative reward:  -38.566552901024345\n",
            "current epsilon:  0.536268225207185\n",
            "#---------------------------------------------#\n",
            "episode  62\n",
            "episode cumulative reward:  -37.7162629757789\n",
            "current epsilon:  0.5309055429551132\n",
            "#---------------------------------------------#\n",
            "episode  63\n",
            "episode cumulative reward:  39.28571428571391\n",
            "current epsilon:  0.525596487525562\n",
            "#---------------------------------------------#\n",
            "episode  64\n",
            "episode cumulative reward:  -31.59609120521227\n",
            "current epsilon:  0.5203405226503064\n",
            "#---------------------------------------------#\n",
            "Save weigths in: Checkpoints/checkpoint.pth.tar\n",
            "Target network updated\n",
            "episode  65\n",
            "episode cumulative reward:  -11.26279863481307\n",
            "current epsilon:  0.5151371174238033\n",
            "#---------------------------------------------#\n",
            "episode  66\n",
            "episode cumulative reward:  -120.72499999999997\n",
            "current epsilon:  0.5099857462495653\n",
            "#---------------------------------------------#\n",
            "episode  67\n",
            "episode cumulative reward:  -9.87124463519389\n",
            "current epsilon:  0.5048858887870696\n",
            "#---------------------------------------------#\n",
            "episode  68\n",
            "episode cumulative reward:  -48.97959183673527\n",
            "current epsilon:  0.4998370298991989\n",
            "#---------------------------------------------#\n",
            "episode  69\n",
            "episode cumulative reward:  -56.81063122923666\n",
            "current epsilon:  0.49483865960020695\n",
            "#---------------------------------------------#\n",
            "Save weigths in: Checkpoints/checkpoint.pth.tar\n",
            "Target network updated\n",
            "episode  70\n",
            "episode cumulative reward:  -67.10526315789515\n",
            "current epsilon:  0.4898902730042049\n",
            "#---------------------------------------------#\n",
            "episode  71\n",
            "episode cumulative reward:  -74.68354430379746\n",
            "current epsilon:  0.48499137027416284\n",
            "#---------------------------------------------#\n",
            "episode  72\n",
            "episode cumulative reward:  -38.93129770992441\n",
            "current epsilon:  0.4801414565714212\n",
            "#---------------------------------------------#\n",
            "episode  73\n",
            "episode cumulative reward:  -22.261484098939942\n",
            "current epsilon:  0.475340042005707\n",
            "#---------------------------------------------#\n",
            "episode  74\n",
            "episode cumulative reward:  -26.984126984127403\n",
            "current epsilon:  0.47058664158564995\n",
            "#---------------------------------------------#\n",
            "Save weigths in: Checkpoints/checkpoint.pth.tar\n",
            "Target network updated\n",
            "episode  75\n",
            "episode cumulative reward:  -26.923076923076948\n",
            "current epsilon:  0.4658807751697934\n",
            "#---------------------------------------------#\n",
            "episode  76\n",
            "episode cumulative reward:  -44.636678200692614\n",
            "current epsilon:  0.4612219674180955\n",
            "#---------------------------------------------#\n",
            "episode  77\n",
            "episode cumulative reward:  -52.70270270270342\n",
            "current epsilon:  0.45660974774391455\n",
            "#---------------------------------------------#\n",
            "episode  78\n",
            "episode cumulative reward:  -33.7748344370867\n",
            "current epsilon:  0.4520436502664754\n",
            "#---------------------------------------------#\n",
            "episode  79\n",
            "episode cumulative reward:  -62.02531645569684\n",
            "current epsilon:  0.44752321376381066\n",
            "#---------------------------------------------#\n",
            "Save weigths in: Checkpoints/checkpoint.pth.tar\n",
            "Target network updated\n",
            "episode  80\n",
            "episode cumulative reward:  -48.096885813149285\n",
            "current epsilon:  0.44304798162617254\n",
            "#---------------------------------------------#\n",
            "episode  81\n",
            "episode cumulative reward:  -46.84385382059866\n",
            "current epsilon:  0.4386175018099108\n",
            "#---------------------------------------------#\n",
            "episode  82\n",
            "episode cumulative reward:  -27.00729927007292\n",
            "current epsilon:  0.4342313267918117\n",
            "#---------------------------------------------#\n",
            "episode  83\n",
            "episode cumulative reward:  -22.155688622754603\n",
            "current epsilon:  0.4298890135238936\n",
            "#---------------------------------------------#\n",
            "episode  84\n",
            "episode cumulative reward:  -46.127946127946494\n",
            "current epsilon:  0.42559012338865465\n",
            "#---------------------------------------------#\n",
            "Save weigths in: Checkpoints/checkpoint.pth.tar\n",
            "Target network updated\n",
            "episode  85\n",
            "episode cumulative reward:  -57.18654434250815\n",
            "current epsilon:  0.4213342221547681\n",
            "#---------------------------------------------#\n",
            "episode  86\n",
            "episode cumulative reward:  -38.566552901024444\n",
            "current epsilon:  0.41712087993322045\n",
            "#---------------------------------------------#\n",
            "episode  87\n",
            "episode cumulative reward:  -49.01960784313803\n",
            "current epsilon:  0.41294967113388825\n",
            "#---------------------------------------------#\n",
            "episode  88\n",
            "episode cumulative reward:  -22.297297297297835\n",
            "current epsilon:  0.40882017442254937\n",
            "#---------------------------------------------#\n",
            "episode  89\n",
            "episode cumulative reward:  -63.76811594202971\n",
            "current epsilon:  0.4047319726783239\n",
            "#---------------------------------------------#\n",
            "Save weigths in: Checkpoints/checkpoint.pth.tar\n",
            "Target network updated\n",
            "episode  90\n",
            "episode cumulative reward:  -25.000000000000394\n",
            "current epsilon:  0.40068465295154065\n",
            "#---------------------------------------------#\n",
            "episode  91\n",
            "episode cumulative reward:  -13.461538461538968\n",
            "current epsilon:  0.39667780642202527\n",
            "#---------------------------------------------#\n",
            "episode  92\n",
            "episode cumulative reward:  -53.020134228188354\n",
            "current epsilon:  0.392711028357805\n",
            "#---------------------------------------------#\n",
            "episode  93\n",
            "episode cumulative reward:  -55.47945205479504\n",
            "current epsilon:  0.38878391807422696\n",
            "#---------------------------------------------#\n",
            "episode  94\n",
            "episode cumulative reward:  -3.0100334448163037\n",
            "current epsilon:  0.3848960788934847\n",
            "#---------------------------------------------#\n",
            "Save weigths in: Checkpoints/checkpoint.pth.tar\n",
            "Target network updated\n",
            "episode  95\n",
            "episode cumulative reward:  -23.076923076923112\n",
            "current epsilon:  0.38104711810454983\n",
            "#---------------------------------------------#\n",
            "episode  96\n",
            "episode cumulative reward:  -59.73154362416189\n",
            "current epsilon:  0.37723664692350434\n",
            "#---------------------------------------------#\n",
            "episode  97\n",
            "episode cumulative reward:  -31.034482758621238\n",
            "current epsilon:  0.37346428045426927\n",
            "#---------------------------------------------#\n",
            "episode  98\n",
            "episode cumulative reward:  -148.72597402597438\n",
            "current epsilon:  0.36972963764972655\n",
            "#---------------------------------------------#\n",
            "episode  99\n",
            "episode cumulative reward:  -42.56756756756793\n",
            "current epsilon:  0.36603234127322926\n",
            "#---------------------------------------------#\n",
            "Save weigths in: Checkpoints/checkpoint.pth.tar\n",
            "Target network updated\n",
            "episode  100\n",
            "episode cumulative reward:  -64.91228070175508\n",
            "current epsilon:  0.36237201786049694\n",
            "#---------------------------------------------#\n",
            "episode  101\n",
            "episode cumulative reward:  -71.11913357400746\n",
            "current epsilon:  0.358748297681892\n",
            "#---------------------------------------------#\n",
            "episode  102\n",
            "episode cumulative reward:  -39.68253968254037\n",
            "current epsilon:  0.35516081470507305\n",
            "#---------------------------------------------#\n",
            "episode  103\n",
            "episode cumulative reward:  -67.03296703296755\n",
            "current epsilon:  0.3516092065580223\n",
            "#---------------------------------------------#\n",
            "episode  104\n",
            "episode cumulative reward:  -54.22535211267685\n",
            "current epsilon:  0.34809311449244207\n",
            "#---------------------------------------------#\n",
            "Save weigths in: Checkpoints/checkpoint.pth.tar\n",
            "Target network updated\n",
            "episode  105\n",
            "episode cumulative reward:  -65.18987341772198\n",
            "current epsilon:  0.34461218334751764\n",
            "#---------------------------------------------#\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCZDvbZaAKye"
      },
      "source": [
        "## Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "QUl59hUJANVv",
        "outputId": "1b59d75a-80f0-4d09-cd2b-9e73e00b1cba"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-f8c41a22e6c0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgnwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAnimation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCarRacing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCarRacing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrender_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rgb_array\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#env = CarRacing(render_mode=\"state_pixels\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#env = gym.make(\"CarRacing-v2\", domain_randomize=True, render_mode=\"state_pixels\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'CarRacing' is not defined"
          ]
        }
      ],
      "source": [
        "env = gnwrapper.Animation(CarRacing())\n",
        "env = CarRacing(render_mode=\"rgb_array\")\n",
        "#env = CarRacing(render_mode=\"state_pixels\")\n",
        "#env = gym.make(\"CarRacing-v2\", domain_randomize=True, render_mode=\"state_pixels\")\n",
        "\n",
        "env.reset()\n",
        "#env.render()\n",
        "#im = env.render()\n",
        "#im = env.render(\"state_pixels\")\n",
        "\n",
        "#plt.imshow(im)\n",
        "\n",
        "for i in range(40):\n",
        "  env.step(action=[-0.3,1,0])\n",
        "  im = env.render()\n",
        "  plt.imshow(im)\n",
        "\n",
        "\"\"\"\n",
        "def state_image_preprocess(state_image):\n",
        "    state_image = state_image.transpose((2,0,1))\n",
        "    state_image = np.ascontiguousarray(state_image, dtype=np.float32) / 255\n",
        "    state_image = torch.from_numpy(state_image)\n",
        "    return state_image.unsqueeze(0).to(device)\n",
        "\n",
        "state_image_preprocess(im).shape\n",
        "plt.imshow(state_image_preprocess(im).cpu().squeeze(0).permute(1, 2, 0).numpy())\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}