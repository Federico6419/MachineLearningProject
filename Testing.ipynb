{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDsmMMfcuBpOL445WJ9q3l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Federico6419/MachineLearningProject/blob/main/Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4Mcql4zq7EK"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch import Tensor\n",
        "from collections import deque\n",
        "import config\n",
        "from model import Model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "avarage_random = np.zeros(config.NUM_EPISODES_TEST) # avarage reward for each episode\n",
        "avarage_table = np.zeros(config.NUM_EPISODES_TEST)\n",
        "avarage_nn = np.zeros(config.NUM_EPISODES_TEST)\n",
        "cum_reward_random = np.zeros(config.NUM_EPISODES_TEST) ## cumulative reward for each episode\n",
        "cum_reward_table = np.zeros(config.NUM_EPISODES_TEST)\n",
        "cum_reward_nn = np.zeros(config.NUM_EPISODES_TEST)\n",
        "\n",
        "action_space = [\n",
        "                (-1, 1, 0.2), (0, 1, 0.2), (1, 1, 0.2),\n",
        "                (-1, 1,   0), (0, 1,   0), (1, 1,   0),               #(Steering Wheel, Gas, Break)\n",
        "                (-1, 0, 0.2), (0, 0, 0.2), (1, 0, 0.2),               #Range -1~1 0~1 0~1\n",
        "                (-1, 0,   0), (0, 0,   0), (1, 0,   0)\n",
        "              ]\n",
        "\n",
        "# define the model and load the weights\n",
        "model = Model().to(config.DEVICE)\n",
        "optimizer = optim.Adam(model.parameters(), lr=config.LR)\n",
        "config.load_model(config.CHECKPOINT,model,optimizer)\n",
        "\n",
        "#load the q table csv file\n",
        "#Q = np.loadtxt('q_table_little_discretization2000.csv', delimiter=',')\n",
        "\n",
        "## define the policy\n",
        "def policy(pol,state):\n",
        "    #if(pol == \"random\"):\n",
        "    #    return env.action_space.sample()\n",
        "    #elif(pol == \"q_table\"):\n",
        "    #    return np.argmax(Q[state])\n",
        "    #elif(pol == \"nn\"):\n",
        "    prediction = model(torch.from_numpy(state.astype('float32')).to(config.DEVICE)).detach().cpu().numpy()\n",
        "    action = action_space[np.argmax(prediction)]              #Select the action with the maximum predicted Q-Value\n",
        "\n",
        "    return action\n",
        "\n",
        "\n",
        "env = gym.make(\"CarRacing-v2\", render_mode=\"human\")\n",
        "observation, info = env.reset()\n",
        "\n",
        "###Testing\n",
        "def test(types):\n",
        "    c_reward = 0 #cumulative reward\n",
        "    total_reward = 0\n",
        "    episode = 0\n",
        "    #for i in range(config.NUM_EPISODES_TEST):\n",
        "    for i in range(10):\n",
        "            observation, info = env.reset()\n",
        "            for _ in range(250):\n",
        "                env.render()\n",
        "                #if(types == \"random policy\"):\n",
        "                #    action = policy(\"random\",observation)  # this is where you would insert your policy\n",
        "                #elif(types == \"q table policy\"):\n",
        "                #    state= config.discretize(observation)\n",
        "                #    action = policy(\"q_table\",state)\n",
        "                #elif(types == \"neural network policy\"):\n",
        "                action = policy(\"nn\",observation)\n",
        "\n",
        "                observation, reward, done, truncated, info = env.step(action)\n",
        "                c_reward += reward\n",
        "\n",
        "\n",
        "                if done or truncated:\n",
        "                    total_reward += c_reward\n",
        "                    break\n",
        "\n",
        "            #if(types == \"random policy\"):\n",
        "            #    cum_reward_random[episode] = c_reward # save the cum_reward for the relative episode\n",
        "            #    avarage_random[episode]=total_reward/episode\n",
        "            #elif(types == \"q table policy\"):\n",
        "            #    cum_reward_table[episode] = c_reward\n",
        "            #    avarage_table[episode]=total_reward/episode\n",
        "            #elif(types == \"neural network policy\"):\n",
        "            cum_reward_nn[episode] = c_reward\n",
        "            avarage_nn[episode]=total_reward/episode\n",
        "\n",
        "            print(\"The cumulative reward is:\",c_reward)\n",
        "            c_reward = 0 # reset the current cumulative reward\n",
        "            print(\"Episode: \",episode)\n",
        "            episode += +1\n",
        "\n",
        "    print(f\"Tests for {types} finished after {config.NUM_EPISODES_TEST} episodes\")\n",
        "\n",
        "\n",
        "\n",
        "#test(\"random policy\")\n",
        "#test(\"q table policy\")\n",
        "#test(\"neural network policy\")\n",
        "\n",
        "env.close()"
      ]
    }
  ]
}